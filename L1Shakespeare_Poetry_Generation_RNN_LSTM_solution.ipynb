{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"15XaoR1KkwVINqIG4JEKB8k04YD23cRMm","timestamp":1697178632140},{"file_id":"https://github.com/ccc-frankfurt/Practical_ML_SS21/blob/main/week06/Shakespeare_Poetry_Generation_RNN_LSTM_solution.ipynb","timestamp":1697113658073}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"G1c42fl7aIyp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697189999601,"user_tz":-330,"elapsed":1846,"user":{"displayName":"VARUN CHINDAGE (RA2111047010224)","userId":"15052075973333773834"}},"outputId":"4b2a2b9e-60ab-4549-bd88-ac911f6286c5"},"source":["!pip install torch==1.4.0"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.4.0 (from versions: 1.11.0, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 2.0.0, 2.0.1, 2.1.0)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for torch==1.4.0\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","metadata":{"id":"kinYD3010vdt","executionInfo":{"status":"ok","timestamp":1697190003902,"user_tz":-330,"elapsed":4305,"user":{"displayName":"VARUN CHINDAGE (RA2111047010224)","userId":"15052075973333773834"}}},"source":["import math\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.nn import Parameter\n","\n","# Check whether GPU is available and can be used\n","# if CUDA is found then device is set accordingly\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","if not torch.cuda.is_available():\n","    print(\"Consider changing your run-time to GPU or training will be slow.\")"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"stneSw5L77Ln"},"source":["## The data: Shakespeare's sonnets\n","\n","Shakespeare's sonnets can be found at the following URL featuring all of his works: http://shakespeare.mit.edu/\n","\n","For convenience reasons we have extracted all the plain text of the sonnets: https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt into a separate textfile and have added it to the class' repository. We will thus download it from there:"]},{"cell_type":"code","metadata":{"id":"eTxp46sNyKNQ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1490e8ec-3538-4cf3-9719-b9e4bab44d50","executionInfo":{"status":"ok","timestamp":1697190003902,"user_tz":-330,"elapsed":14,"user":{"displayName":"VARUN CHINDAGE (RA2111047010224)","userId":"15052075973333773834"}}},"source":["!wget https://raw.githubusercontent.com/ccc-frankfurt/Practical_ML_SS21/master/week06/sonnets.txt"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["--2023-10-13 09:40:02--  https://raw.githubusercontent.com/ccc-frankfurt/Practical_ML_SS21/master/week06/sonnets.txt\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 94081 (92K) [text/plain]\n","Saving to: ‘sonnets.txt’\n","\n","sonnets.txt         100%[===================>]  91.88K  --.-KB/s    in 0.004s  \n","\n","2023-10-13 09:40:03 (21.8 MB/s) - ‘sonnets.txt’ saved [94081/94081]\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"wzimDdxq8oXk"},"source":["We can open the text file and print an excerpt."]},{"cell_type":"code","metadata":{"id":"fdSreQlxy11g","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e44a88e6-bdf9-47d1-90c4-e2ae2da300f8","executionInfo":{"status":"ok","timestamp":1697190003902,"user_tz":-330,"elapsed":9,"user":{"displayName":"VARUN CHINDAGE (RA2111047010224)","userId":"15052075973333773834"}}},"source":["# Open shakespeare text file and read the data\n","with open('sonnets.txt', 'r') as f:\n","    text = f.read()\n","\n","# print an excerpt of the text\n","print(text[:200])"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["From fairest creatures we desire increase,\n","That thereby beauty's rose might never die,\n","But as the riper should by time decease,\n","His tender heir might bear his memory:\n","But thou contracted to thine own \n"]}]},{"cell_type":"markdown","metadata":{"id":"CqofvzDN8u6Z"},"source":["As we are interested in a character based neural network, we will now create a mapping from the characters to numbers so that we can do our matrix calculations with numerical data. One such way is to simply replace every character with the corresponding integer in an alphabetical sequence. If we print our excerpt, we can now see the corresponding numerical values of each character."]},{"cell_type":"code","metadata":{"id":"rS8C0ndK0tce","colab":{"base_uri":"https://localhost:8080/"},"outputId":"981cf042-211a-430a-e3d5-2c04ae15bc55","executionInfo":{"status":"ok","timestamp":1697190003903,"user_tz":-330,"elapsed":8,"user":{"displayName":"VARUN CHINDAGE (RA2111047010224)","userId":"15052075973333773834"}}},"source":["# We create two dictionaries:\n","# 1. int2char, which maps integers to characters\n","# 2. char2int, which maps characters to integers\n","chars = tuple(set(text))\n","int2char = dict(enumerate(chars))\n","char2int = {ch: ii for ii, ch in int2char.items()}\n","\n","# Encode the text\n","encoded = np.array([char2int[ch] for ch in text])\n","\n","# Again showing the excerpt, but this time as integers\n","encoded[:200]"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 5, 59, 29, 24, 23, 34, 36,  8, 59, 55, 37, 20, 23, 16, 59, 55, 36,\n","       20, 21, 59, 55, 37, 23, 49, 55, 23, 32, 55, 37,  8, 59, 55, 23,  8,\n","       17, 16, 59, 55, 36, 37, 55, 53, 27,  6, 10, 36, 20, 23, 20, 10, 55,\n","       59, 55, 25, 30, 23, 25, 55, 36, 21, 20, 30, 41, 37, 23, 59, 29, 37,\n","       55, 23, 24,  8, 50, 10, 20, 23, 17, 55, 57, 55, 59, 23, 32,  8, 55,\n","       53, 27,  0, 21, 20, 23, 36, 37, 23, 20, 10, 55, 23, 59,  8,  9, 55,\n","       59, 23, 37, 10, 29, 21, 13, 32, 23, 25, 30, 23, 20,  8, 24, 55, 23,\n","       32, 55, 16, 55, 36, 37, 55, 53, 27, 54,  8, 37, 23, 20, 55, 17, 32,\n","       55, 59, 23, 10, 55,  8, 59, 23, 24,  8, 50, 10, 20, 23, 25, 55, 36,\n","       59, 23, 10,  8, 37, 23, 24, 55, 24, 29, 59, 30, 46, 27,  0, 21, 20,\n","       23, 20, 10, 29, 21, 23, 16, 29, 17, 20, 59, 36, 16, 20, 55, 32, 23,\n","       20, 29, 23, 20, 10,  8, 17, 55, 23, 29, 49, 17, 23])"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"dZCdTzB39dNl"},"source":["### Data loader: batching\n","\n","We now have our entire text file encoded as integers, which serves as our dataset. Next, we will need to define our data loader, mainly the part that is missing, the random sampling of batches. Let us define this method:"]},{"cell_type":"code","metadata":{"id":"ZWq80TVX1DNo","executionInfo":{"status":"ok","timestamp":1697190003903,"user_tz":-330,"elapsed":6,"user":{"displayName":"VARUN CHINDAGE (RA2111047010224)","userId":"15052075973333773834"}}},"source":["# Defining method to make mini-batches for training\n","def get_batches(arr, batch_size, seq_length):\n","    # determine the flattened batch size, i.e. sequence length times batch size\n","    batch_size_total = batch_size * seq_length\n","    # total number of batches we can make\n","    n_batches = len(arr)//batch_size_total\n","\n","    # Keep only enough characters to make full batches\n","    arr = arr[:n_batches * batch_size_total]\n","    # Reshape into batch_size rows\n","    arr = arr.reshape((batch_size, -1))\n","\n","    # iterate through the array, one sequence at a time\n","    for n in range(0, arr.shape[1], seq_length):\n","        # The features\n","        x = arr[:, n:n+seq_length]\n","        # The targets\n","        y = np.zeros_like(x)\n","        try:\n","            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n","        except IndexError:\n","            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n","        yield x, y"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FWDjvcQW92ze"},"source":["### Targets/Labels\n","\n","We will be treating our problem as a classification task, where given an input the task is to predict the likelihood of the next character, i.e. we choose the class/character with the highest probability of a SoftMax output. Our model's output is thus a vector containing a probability for each unique character.\n","\n","Since we want to be able to feed our model's output back as input for the next time step, we should also give the network a one-hot encoded character as the input instead of just an integer, similar to what we have seen on our lecture's last slide.\n","This way the network gets as input a one-hot vector of length corresponding to the number of total unique characters and predicts the likelihood for each character as output (for the next character in the sequence). We will thus write a function that converts our encoded characters from integers to one-hot vectors."]},{"cell_type":"code","metadata":{"id":"6l9QohlqnU-B","executionInfo":{"status":"ok","timestamp":1697190003903,"user_tz":-330,"elapsed":6,"user":{"displayName":"VARUN CHINDAGE (RA2111047010224)","userId":"15052075973333773834"}}},"source":["def one_hot_encode(arr, n_labels):\n","\n","    # Initialize the the encoded array\n","    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n","\n","    # Fill the appropriate elements with ones\n","    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n","\n","    # Finally reshape it to get back to the original array\n","    one_hot = one_hot.reshape((*arr.shape, n_labels))\n","\n","    return one_hot"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0GHm8hBaALrD"},"source":["## A simple RNN\n","\n","We will start with writing a simple RNN in PyTorch. To get a better understanding of how the RNN model works, we will not be using PyTorch's convenience RNN implementation, but write the main portion by hand ourselves. We will later use the convenience functions for the much more complicated LSTMs.\n","\n","Note that we could in principle do the same thing in pure Numpy but the advantage of implementing the forward logic in PyTorch is that we can use the automatic differentation for our backward pass and we do not need to implement the backpropagation through time ourselves.\n","\n","What we will learn here is:\n","1. How to write a recurrent neural network (the forward pass)\n","2. How to implement custom mathematical equations in the forward pass of a PyTorch model and leverage the automatic backward."]},{"cell_type":"code","metadata":{"id":"Cgkv_TmovgBY","executionInfo":{"status":"ok","timestamp":1697190003903,"user_tz":-330,"elapsed":6,"user":{"displayName":"VARUN CHINDAGE (RA2111047010224)","userId":"15052075973333773834"}}},"source":["class RNN(nn.Module):\n","    def __init__(self, chars, device, hidden_sz, drop_prob=0.5):\n","        super().__init__()\n","\n","        self.device = device\n","\n","        # creating character dictionaries\n","        # we already have this code on the top, but giving it to our model\n","        # will be convenient for doing predictions later\n","        # i.e. doing conversions from text to integers to one-hot & vice-versa\n","        self.n_chars = len(chars)\n","        self.int2char = dict(enumerate(chars))\n","        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n","\n","        self.hidden_sz = hidden_sz\n","\n","        # Note that this class inherits from the torch neural network class\n","        # Instead of using a pre-built function we will write the math ourselves\n","        # For this reason we will first need to define \"Parameters()\", that\n","        # the PyTorch graph keeps track of and can optimize. In other words,\n","        # let's give our class the weights & the bias that the RNN will need.\n","        self.weight_ih = Parameter(torch.Tensor(self.n_chars, self.hidden_sz))\n","        self.weight_hh = Parameter(torch.Tensor(self.hidden_sz, self.hidden_sz))\n","        self.bias_hh = Parameter(torch.Tensor(self.hidden_sz))\n","\n","        # Now that we have defined the RNN cell, let us define the output layer\n","        # We will use a dropout layer to prevent overfitting and then\n","        # follow with a conventional linear layer (matrix multiplication) that\n","        # maps the RNN cell's output (the hidden state of the network) to the\n","        # class output. Remembert that the class output corresponds to a\n","        # vector of length of unique characters.\n","\n","        # define a dropout layer\n","        self.dropout = nn.Dropout(drop_prob)\n","\n","        # define the final, fully-connected output layer. We can use a\n","        # PyTorch nn function here (or you could add the corresponding math\n","        # below and assign an additional weight & bias at the top).\n","        # We can see that we can create very custom models this way\n","        self.fc = nn.Linear(self.hidden_sz, self.n_chars)\n","\n","        # We have assigned the Parameters above, but we will need to also\n","        # initialize them. Let's write a function for that and initialize\n","        # our weights and bias.\n","        self.init_weights()\n","\n","    def init_weights(self):\n","        nn.init.xavier_uniform_(self.weight_ih)\n","        nn.init.xavier_uniform_(self.weight_hh)\n","        nn.init.zeros_(self.bias_hh)\n","\n","    def forward(self, x, h_t):\n","        \"\"\"Assumes x is of shape (batch, sequence, feature)\"\"\"\n","        bs, seq_sz, _ = x.size()\n","        hidden_seq = []\n","\n","        # Given an input and an initial hidden state, calculate the next hidden\n","        # state for each sequence element.\n","        # We append all the hidden states to a list (similar to a batch size)\n","        # so that we can concatenate them in the batch and feed them to our\n","        # last linear layer all in parallel to avoid looping through the final\n","        # output layer as there is no more dependence on other time steps.\n","        for t in range(seq_sz):\n","            x_t = x[:, t, :]\n","            h_t = torch.tanh(x_t @ self.weight_ih + h_t @ self.weight_hh + self.bias_hh)\n","            hidden_seq.append(h_t.unsqueeze(0))\n","\n","        # Do the concatenation and reshaping for convenience\n","        hidden_seq = torch.cat(hidden_seq, dim=0)\n","        # reshape from shape (sequence, batch, feature) to (batch, sequence, feature)\n","        hidden_seq = hidden_seq.transpose(0, 1).contiguous()\n","\n","        # Stack up the RNN outputs using view so that we can process the last\n","        # layer in parallel\n","        r_output = hidden_seq.contiguous().view(-1, self.hidden_sz)\n","\n","        # pass through a dropout layer\n","        out = self.dropout(r_output)\n","\n","        # Calculate fully connected layer output that yields our class vector\n","        out = self.fc(out)\n","\n","        return out, h_t\n","\n","    def init_hidden(self, batch_size=1):\n","        ''' Initializes hidden state '''\n","        # This is a convenience function so that we can initialize a hidden\n","        # state to zero when we start prediction on a sequence. Every further\n","        # step will then depend on the previous hidden state.\n","\n","        # Create two new tensors with sizes batch_size x n_hidden,\n","        # initialized to zero for hidden the RNN's hidden state.\n","        weight = next(self.parameters()).data\n","        h_t = weight.new(batch_size, self.hidden_sz).zero_().to(device)\n","\n","        return h_t\n"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UhNyXgWuFK2T"},"source":["The only thing missing is our training loop. It will look very similar to everything we have previously written, with two main differences:\n","\n","1. Our model is now also dependent on the hidden state and thus takes it as input and returns it as an additional output.\n","2. Because we are using a recurrent neural network we will need to give our \"loss.backward()\" a \"retain_graph=True\" flag in order for it to log the history and be able to compute the backpropagation through time"]},{"cell_type":"code","metadata":{"id":"LSrhA4lR2EaP","executionInfo":{"status":"ok","timestamp":1697190003903,"user_tz":-330,"elapsed":5,"user":{"displayName":"VARUN CHINDAGE (RA2111047010224)","userId":"15052075973333773834"}}},"source":["# Declaring the train method\n","def train(model, data, device, optimizer, criterion, epochs=10, batch_size=10,\n","          seq_length=50, clip=5):\n","    model.train()\n","\n","    for epoch in range(epochs):\n","        # initialize first hidden states with zeros\n","        h = model.init_hidden(batch_size)\n","\n","        for x, y in get_batches(data, batch_size, seq_length):\n","            # One-hot encode our data, make them torch tensors & cast to device\n","            x = one_hot_encode(x, model.n_chars)\n","            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n","            inputs, targets = inputs.to(device), targets.to(device)\n","\n","            # zero accumulated gradients\n","            model.zero_grad()\n","\n","            # get the output and hidden state from the model\n","            output, h = model(inputs, h)\n","\n","            # calculate the loss and perform backprop\n","            # because we have flattened our batch and sequence in the model to\n","            # be able to speed up the connection of the last fully-connected\n","            # layer we now also need to view/flatten our target here\n","            loss = criterion(output, targets.view(batch_size*seq_length).long())\n","            loss.backward(retain_graph=True)\n","\n","            # we use an additional trick of clipping gradients to avoid\n","            # exploding gradients, which is a prominent problem in RNNs, just\n","            # as the opposite problem of vanishing gradients.\n","            nn.utils.clip_grad_norm_(model.parameters(), clip)\n","            optimizer.step()\n","\n","\n","        print(\"Epoch: {}/{}:\".format(epoch + 1, epochs),\n","              \"Loss: {:.4f}:\".format(loss.item()))"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"FFKl2jWi2GeC","colab":{"base_uri":"https://localhost:8080/"},"outputId":"18a607b6-f5b1-4a50-fceb-51995e21d4de","executionInfo":{"status":"ok","timestamp":1697190152787,"user_tz":-330,"elapsed":148889,"user":{"displayName":"VARUN CHINDAGE (RA2111047010224)","userId":"15052075973333773834"}}},"source":["# Define the model\n","n_hidden=512\n","model = RNN(chars, device, n_hidden).to(device)\n","\n","# Hyperparameters\n","batch_size = 128\n","seq_length = 100\n","epochs = 300 # start with 50 or similar if you are debugging\n","# train much longer if you want good results\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","criterion = nn.CrossEntropyLoss()\n","\n","# train the model\n","train(model, encoded, device, optimizer, criterion, epochs=epochs,\n","      batch_size=batch_size, seq_length=seq_length)"],"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1/300: Loss: 3.4386:\n","Epoch: 2/300: Loss: 3.1921:\n","Epoch: 3/300: Loss: 5.7946:\n","Epoch: 4/300: Loss: 3.4981:\n","Epoch: 5/300: Loss: 3.3691:\n","Epoch: 6/300: Loss: 3.3049:\n","Epoch: 7/300: Loss: 3.2798:\n","Epoch: 8/300: Loss: 3.2544:\n","Epoch: 9/300: Loss: 3.2269:\n","Epoch: 10/300: Loss: 3.1941:\n","Epoch: 11/300: Loss: 3.0739:\n","Epoch: 12/300: Loss: 3.0447:\n","Epoch: 13/300: Loss: 2.9946:\n","Epoch: 14/300: Loss: 2.9444:\n","Epoch: 15/300: Loss: 2.8986:\n","Epoch: 16/300: Loss: 2.8606:\n","Epoch: 17/300: Loss: 2.8207:\n","Epoch: 18/300: Loss: 2.7862:\n","Epoch: 19/300: Loss: 2.7512:\n","Epoch: 20/300: Loss: 2.7498:\n","Epoch: 21/300: Loss: 2.6979:\n","Epoch: 22/300: Loss: 2.6548:\n","Epoch: 23/300: Loss: 2.6166:\n","Epoch: 24/300: Loss: 2.6056:\n","Epoch: 25/300: Loss: 2.5682:\n","Epoch: 26/300: Loss: 2.5303:\n","Epoch: 27/300: Loss: 2.5071:\n","Epoch: 28/300: Loss: 2.4969:\n","Epoch: 29/300: Loss: 2.4582:\n","Epoch: 30/300: Loss: 2.4323:\n","Epoch: 31/300: Loss: 2.4266:\n","Epoch: 32/300: Loss: 2.4097:\n","Epoch: 33/300: Loss: 2.3879:\n","Epoch: 34/300: Loss: 2.3731:\n","Epoch: 35/300: Loss: 2.3565:\n","Epoch: 36/300: Loss: 2.3392:\n","Epoch: 37/300: Loss: 2.3248:\n","Epoch: 38/300: Loss: 2.3131:\n","Epoch: 39/300: Loss: 2.3254:\n","Epoch: 40/300: Loss: 2.2983:\n","Epoch: 41/300: Loss: 2.2807:\n","Epoch: 42/300: Loss: 2.2712:\n","Epoch: 43/300: Loss: 2.2549:\n","Epoch: 44/300: Loss: 2.2472:\n","Epoch: 45/300: Loss: 2.2346:\n","Epoch: 46/300: Loss: 2.2433:\n","Epoch: 47/300: Loss: 2.2242:\n","Epoch: 48/300: Loss: 2.2314:\n","Epoch: 49/300: Loss: 2.2083:\n","Epoch: 50/300: Loss: 2.1973:\n","Epoch: 51/300: Loss: 2.2009:\n","Epoch: 52/300: Loss: 2.1924:\n","Epoch: 53/300: Loss: 2.1814:\n","Epoch: 54/300: Loss: 2.2161:\n","Epoch: 55/300: Loss: 2.2196:\n","Epoch: 56/300: Loss: 2.1924:\n","Epoch: 57/300: Loss: 2.1741:\n","Epoch: 58/300: Loss: 2.1603:\n","Epoch: 59/300: Loss: 2.1497:\n","Epoch: 60/300: Loss: 2.1399:\n","Epoch: 61/300: Loss: 2.1368:\n","Epoch: 62/300: Loss: 2.1379:\n","Epoch: 63/300: Loss: 2.1224:\n","Epoch: 64/300: Loss: 2.1218:\n","Epoch: 65/300: Loss: 2.1116:\n","Epoch: 66/300: Loss: 2.1174:\n","Epoch: 67/300: Loss: 2.1082:\n","Epoch: 68/300: Loss: 2.0989:\n","Epoch: 69/300: Loss: 2.0935:\n","Epoch: 70/300: Loss: 2.0850:\n","Epoch: 71/300: Loss: 2.0754:\n","Epoch: 72/300: Loss: 2.1039:\n","Epoch: 73/300: Loss: 2.0905:\n","Epoch: 74/300: Loss: 2.0744:\n","Epoch: 75/300: Loss: 2.0631:\n","Epoch: 76/300: Loss: 2.0674:\n","Epoch: 77/300: Loss: 2.0727:\n","Epoch: 78/300: Loss: 2.0542:\n","Epoch: 79/300: Loss: 2.0527:\n","Epoch: 80/300: Loss: 2.0390:\n","Epoch: 81/300: Loss: 2.0390:\n","Epoch: 82/300: Loss: 2.0306:\n","Epoch: 83/300: Loss: 2.0314:\n","Epoch: 84/300: Loss: 2.0253:\n","Epoch: 85/300: Loss: 2.0104:\n","Epoch: 86/300: Loss: 2.0048:\n","Epoch: 87/300: Loss: 2.0007:\n","Epoch: 88/300: Loss: 2.0023:\n","Epoch: 89/300: Loss: 2.0083:\n","Epoch: 90/300: Loss: 1.9913:\n","Epoch: 91/300: Loss: 1.9925:\n","Epoch: 92/300: Loss: 2.0013:\n","Epoch: 93/300: Loss: 1.9926:\n","Epoch: 94/300: Loss: 1.9835:\n","Epoch: 95/300: Loss: 1.9780:\n","Epoch: 96/300: Loss: 1.9574:\n","Epoch: 97/300: Loss: 1.9597:\n","Epoch: 98/300: Loss: 1.9630:\n","Epoch: 99/300: Loss: 1.9547:\n","Epoch: 100/300: Loss: 1.9502:\n","Epoch: 101/300: Loss: 1.9691:\n","Epoch: 102/300: Loss: 1.9599:\n","Epoch: 103/300: Loss: 1.9425:\n","Epoch: 104/300: Loss: 1.9442:\n","Epoch: 105/300: Loss: 1.9317:\n","Epoch: 106/300: Loss: 1.9195:\n","Epoch: 107/300: Loss: 1.9126:\n","Epoch: 108/300: Loss: 1.9097:\n","Epoch: 109/300: Loss: 1.9054:\n","Epoch: 110/300: Loss: 1.9048:\n","Epoch: 111/300: Loss: 1.9061:\n","Epoch: 112/300: Loss: 1.9126:\n","Epoch: 113/300: Loss: 1.9010:\n","Epoch: 114/300: Loss: 1.8903:\n","Epoch: 115/300: Loss: 1.8915:\n","Epoch: 116/300: Loss: 1.8858:\n","Epoch: 117/300: Loss: 1.8752:\n","Epoch: 118/300: Loss: 1.8736:\n","Epoch: 119/300: Loss: 1.8522:\n","Epoch: 120/300: Loss: 1.8469:\n","Epoch: 121/300: Loss: 1.8530:\n","Epoch: 122/300: Loss: 1.8435:\n","Epoch: 123/300: Loss: 1.8630:\n","Epoch: 124/300: Loss: 1.8690:\n","Epoch: 125/300: Loss: 1.8673:\n","Epoch: 126/300: Loss: 1.8406:\n","Epoch: 127/300: Loss: 1.8551:\n","Epoch: 128/300: Loss: 1.8467:\n","Epoch: 129/300: Loss: 1.8394:\n","Epoch: 130/300: Loss: 1.8394:\n","Epoch: 131/300: Loss: 1.8341:\n","Epoch: 132/300: Loss: 1.8096:\n","Epoch: 133/300: Loss: 1.8104:\n","Epoch: 134/300: Loss: 1.7927:\n","Epoch: 135/300: Loss: 1.8022:\n","Epoch: 136/300: Loss: 1.7805:\n","Epoch: 137/300: Loss: 1.7987:\n","Epoch: 138/300: Loss: 1.7918:\n","Epoch: 139/300: Loss: 1.7864:\n","Epoch: 140/300: Loss: 1.7609:\n","Epoch: 141/300: Loss: 1.7895:\n","Epoch: 142/300: Loss: 1.7706:\n","Epoch: 143/300: Loss: 1.7961:\n","Epoch: 144/300: Loss: 1.7661:\n","Epoch: 145/300: Loss: 1.7522:\n","Epoch: 146/300: Loss: 1.7518:\n","Epoch: 147/300: Loss: 1.7580:\n","Epoch: 148/300: Loss: 1.7714:\n","Epoch: 149/300: Loss: 1.7399:\n","Epoch: 150/300: Loss: 1.7346:\n","Epoch: 151/300: Loss: 1.7388:\n","Epoch: 152/300: Loss: 1.7241:\n","Epoch: 153/300: Loss: 1.7470:\n","Epoch: 154/300: Loss: 1.7345:\n","Epoch: 155/300: Loss: 1.7398:\n","Epoch: 156/300: Loss: 1.7269:\n","Epoch: 157/300: Loss: 1.7231:\n","Epoch: 158/300: Loss: 1.6751:\n","Epoch: 159/300: Loss: 1.7038:\n","Epoch: 160/300: Loss: 1.6854:\n","Epoch: 161/300: Loss: 1.6799:\n","Epoch: 162/300: Loss: 1.6935:\n","Epoch: 163/300: Loss: 1.6858:\n","Epoch: 164/300: Loss: 1.6666:\n","Epoch: 165/300: Loss: 1.6733:\n","Epoch: 166/300: Loss: 1.6526:\n","Epoch: 167/300: Loss: 1.6667:\n","Epoch: 168/300: Loss: 1.6785:\n","Epoch: 169/300: Loss: 1.6878:\n","Epoch: 170/300: Loss: 1.6643:\n","Epoch: 171/300: Loss: 1.6809:\n","Epoch: 172/300: Loss: 1.6391:\n","Epoch: 173/300: Loss: 1.6526:\n","Epoch: 174/300: Loss: 1.6307:\n","Epoch: 175/300: Loss: 1.6285:\n","Epoch: 176/300: Loss: 1.6257:\n","Epoch: 177/300: Loss: 1.6011:\n","Epoch: 178/300: Loss: 1.6428:\n","Epoch: 179/300: Loss: 1.6294:\n","Epoch: 180/300: Loss: 1.6187:\n","Epoch: 181/300: Loss: 1.6207:\n","Epoch: 182/300: Loss: 1.6037:\n","Epoch: 183/300: Loss: 1.6031:\n","Epoch: 184/300: Loss: 1.6225:\n","Epoch: 185/300: Loss: 1.5919:\n","Epoch: 186/300: Loss: 1.5956:\n","Epoch: 187/300: Loss: 1.5985:\n","Epoch: 188/300: Loss: 1.5973:\n","Epoch: 189/300: Loss: 1.6090:\n","Epoch: 190/300: Loss: 1.5968:\n","Epoch: 191/300: Loss: 1.5582:\n","Epoch: 192/300: Loss: 1.5851:\n","Epoch: 193/300: Loss: 1.5583:\n","Epoch: 194/300: Loss: 1.5821:\n","Epoch: 195/300: Loss: 1.6034:\n","Epoch: 196/300: Loss: 1.5748:\n","Epoch: 197/300: Loss: 1.5744:\n","Epoch: 198/300: Loss: 1.5551:\n","Epoch: 199/300: Loss: 1.5801:\n","Epoch: 200/300: Loss: 1.5490:\n","Epoch: 201/300: Loss: 1.5505:\n","Epoch: 202/300: Loss: 1.5759:\n","Epoch: 203/300: Loss: 1.5660:\n","Epoch: 204/300: Loss: 1.5656:\n","Epoch: 205/300: Loss: 1.5544:\n","Epoch: 206/300: Loss: 1.5531:\n","Epoch: 207/300: Loss: 1.5378:\n","Epoch: 208/300: Loss: 1.5437:\n","Epoch: 209/300: Loss: 1.5536:\n","Epoch: 210/300: Loss: 1.5460:\n","Epoch: 211/300: Loss: 1.5420:\n","Epoch: 212/300: Loss: 1.5626:\n","Epoch: 213/300: Loss: 1.5407:\n","Epoch: 214/300: Loss: 1.5319:\n","Epoch: 215/300: Loss: 1.5151:\n","Epoch: 216/300: Loss: 1.5325:\n","Epoch: 217/300: Loss: 1.5173:\n","Epoch: 218/300: Loss: 1.5017:\n","Epoch: 219/300: Loss: 1.5031:\n","Epoch: 220/300: Loss: 1.5071:\n","Epoch: 221/300: Loss: 1.5198:\n","Epoch: 222/300: Loss: 1.4819:\n","Epoch: 223/300: Loss: 1.4631:\n","Epoch: 224/300: Loss: 1.4171:\n","Epoch: 225/300: Loss: 1.4309:\n","Epoch: 226/300: Loss: 1.4385:\n","Epoch: 227/300: Loss: 1.4415:\n","Epoch: 228/300: Loss: 1.4557:\n","Epoch: 229/300: Loss: 1.4658:\n","Epoch: 230/300: Loss: 1.4498:\n","Epoch: 231/300: Loss: 1.4603:\n","Epoch: 232/300: Loss: 1.4806:\n","Epoch: 233/300: Loss: 1.4779:\n","Epoch: 234/300: Loss: 1.4830:\n","Epoch: 235/300: Loss: 1.4876:\n","Epoch: 236/300: Loss: 1.4824:\n","Epoch: 237/300: Loss: 1.4910:\n","Epoch: 238/300: Loss: 1.4801:\n","Epoch: 239/300: Loss: 1.4710:\n","Epoch: 240/300: Loss: 1.4889:\n","Epoch: 241/300: Loss: 1.5006:\n","Epoch: 242/300: Loss: 1.5220:\n","Epoch: 243/300: Loss: 1.4759:\n","Epoch: 244/300: Loss: 1.4544:\n","Epoch: 245/300: Loss: 1.4561:\n","Epoch: 246/300: Loss: 1.4403:\n","Epoch: 247/300: Loss: 1.4483:\n","Epoch: 248/300: Loss: 1.4130:\n","Epoch: 249/300: Loss: 1.4233:\n","Epoch: 250/300: Loss: 1.4241:\n","Epoch: 251/300: Loss: 1.4210:\n","Epoch: 252/300: Loss: 1.4154:\n","Epoch: 253/300: Loss: 1.4197:\n","Epoch: 254/300: Loss: 1.4087:\n","Epoch: 255/300: Loss: 1.4069:\n","Epoch: 256/300: Loss: 1.4060:\n","Epoch: 257/300: Loss: 1.4042:\n","Epoch: 258/300: Loss: 1.3910:\n","Epoch: 259/300: Loss: 1.3964:\n","Epoch: 260/300: Loss: 1.3861:\n","Epoch: 261/300: Loss: 1.3903:\n","Epoch: 262/300: Loss: 1.3751:\n","Epoch: 263/300: Loss: 1.3539:\n","Epoch: 264/300: Loss: 1.3404:\n","Epoch: 265/300: Loss: 1.3410:\n","Epoch: 266/300: Loss: 1.3373:\n","Epoch: 267/300: Loss: 1.3346:\n","Epoch: 268/300: Loss: 1.3465:\n","Epoch: 269/300: Loss: 1.3402:\n","Epoch: 270/300: Loss: 1.3591:\n","Epoch: 271/300: Loss: 1.3473:\n","Epoch: 272/300: Loss: 1.3279:\n","Epoch: 273/300: Loss: 1.3334:\n","Epoch: 274/300: Loss: 1.2991:\n","Epoch: 275/300: Loss: 1.3313:\n","Epoch: 276/300: Loss: 1.2993:\n","Epoch: 277/300: Loss: 1.2981:\n","Epoch: 278/300: Loss: 1.3034:\n","Epoch: 279/300: Loss: 1.3232:\n","Epoch: 280/300: Loss: 1.3258:\n","Epoch: 281/300: Loss: 1.3244:\n","Epoch: 282/300: Loss: 1.3342:\n","Epoch: 283/300: Loss: 1.3195:\n","Epoch: 284/300: Loss: 1.3460:\n","Epoch: 285/300: Loss: 1.3234:\n","Epoch: 286/300: Loss: 1.3181:\n","Epoch: 287/300: Loss: 1.3044:\n","Epoch: 288/300: Loss: 1.3127:\n","Epoch: 289/300: Loss: 1.2913:\n","Epoch: 290/300: Loss: 1.3021:\n","Epoch: 291/300: Loss: 1.2959:\n","Epoch: 292/300: Loss: 1.3093:\n","Epoch: 293/300: Loss: 1.3022:\n","Epoch: 294/300: Loss: 1.3142:\n","Epoch: 295/300: Loss: 1.2995:\n","Epoch: 296/300: Loss: 1.3080:\n","Epoch: 297/300: Loss: 1.2823:\n","Epoch: 298/300: Loss: 1.2868:\n","Epoch: 299/300: Loss: 1.2524:\n","Epoch: 300/300: Loss: 1.2758:\n"]}]},{"cell_type":"markdown","metadata":{"id":"vIndlrMSGMmZ"},"source":["You should observe the loss sinking consistently. In fact you can observe that the model training hasn't fully converged yet. If you feel like you want to spend the time later to see how\n","well you can get your RNN to perform, try training it for longer/until convergence.\n","\n","Once we have trained the model it will be interesting to use it for prediction. To generate new content we would like to feed in an initial sequence or even just a single character and see what the model generates for the rest of the sequence conditioned on our input.\n","\n","Let us write the logic for that:"]},{"cell_type":"code","metadata":{"id":"A53V0aKj2IfT","executionInfo":{"status":"ok","timestamp":1697190152788,"user_tz":-330,"elapsed":24,"user":{"displayName":"VARUN CHINDAGE (RA2111047010224)","userId":"15052075973333773834"}}},"source":["def predict(model, char, device, h=None, top_k=5):\n","        ''' Given a character & hidden state, predict the next character.\n","            Returns the predicted character and the hidden state.\n","        '''\n","\n","        # tensor inputs\n","        x = np.array([[model.char2int[char]]])\n","        x = one_hot_encode(x, model.n_chars)\n","        inputs = torch.from_numpy(x).to(device)\n","\n","        with torch.no_grad():\n","            # get the output of the model\n","            out, h = model(inputs, h)\n","\n","            # get the character probabilities\n","            # move to cpu for further processing with numpy etc.\n","            p = F.softmax(out, dim=1).data.cpu()\n","\n","            # get the top characters with highest likelihood\n","            p, top_ch = p.topk(top_k)\n","            top_ch = top_ch.numpy().squeeze()\n","\n","            # select the likely next character with some element of randomness\n","            # for more variability\n","            p = p.numpy().squeeze()\n","            char = np.random.choice(top_ch, p=p/p.sum())\n","\n","        # return the encoded value of the predicted char and the hidden state\n","        return model.int2char[char], h"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"wHAMT5xTlYY0","executionInfo":{"status":"ok","timestamp":1697190152788,"user_tz":-330,"elapsed":23,"user":{"displayName":"VARUN CHINDAGE (RA2111047010224)","userId":"15052075973333773834"}}},"source":["def sample(model, size, device, prime='A', top_k=None):\n","    # method to generate new text based on a \"prime\"/initial sequence.\n","    # Basically, the outer loop convenience function that calls the above\n","    # defined predict method.\n","    model.eval() # eval mode\n","\n","    # Calculate model for the initial prime characters\n","    chars = [ch for ch in prime]\n","    with torch.no_grad():\n","        # initialize hidden with 0 in the beginning. Set our batch size to 1\n","        # as we wish to generate one sequence only.\n","        h = model.init_hidden(batch_size=1)\n","        for ch in prime:\n","            char, h = predict(model, ch, device, h=h, top_k=top_k)\n","\n","        # append the characters to the sequence\n","        chars.append(char)\n","\n","        # Now pass in the previous/last character and get a new one\n","        # Repeat this process for the desired length of the sequence to be\n","        # generated\n","        for ii in range(size):\n","            char, h = predict(model, chars[-1], device, h=h, top_k=top_k)\n","            chars.append(char)\n","\n","    return ''.join(chars)"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1wgYQs7FH2wo"},"source":["### Generating poems\n","\n","We are now set to call our sample method with our trained model, a prime sequence and a desired sequence length to be generated."]},{"cell_type":"code","metadata":{"id":"HClso847laed","colab":{"base_uri":"https://localhost:8080/"},"outputId":"42799847-3612-4417-c3fa-b1ac763f0335","executionInfo":{"status":"ok","timestamp":1697190153999,"user_tz":-330,"elapsed":1233,"user":{"displayName":"VARUN CHINDAGE (RA2111047010224)","userId":"15052075973333773834"}}},"source":["print(sample(model, 1000, device, prime='A', top_k=5))"],"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["And fat it tosteras hyoven be tine mangre,\n","Whin thit sech al or in mound thin soumys be uty simene tham she whack hh might mise fwert love, te that se presor theme,\n","And ineer priol; wile ighedis canthru tist, whece tor apy ire tort ous rul hivhor word.\n","Or gaot sermughos blane ais elohthea behos, to the seore thou to ghaind.\n","I thy sume foo now,\n","Shell no efa to love watt,\n","I falless baingrids suree thes thace sig thes ishos, dead, but houch in mist,\n","Than aru thot tus seactith, whin thit tewhthts th the re fare trighes me why buin whor dort ste nof an in an my,\n","Sheat duat touress, thou llsmesended ard,\n","Thet with ou bred,\n","So foflly ming this thith the shale thitho reseras, than sw east satt hit tare'd sumy t ofe rlas ost and ewe\n","Whor menit thine oag asl iner shee rofean, then wirlest yos ou dowhere, tok hay me fale, a doth loor.\n","Thee the tir maste duth thatrus dy thee shilld sive the waald the onor il ingives,\n","And saald houls,\n","Steatt mings suthen gonct le ledst tu peris then thas mane ot mone\n"]}]},{"cell_type":"markdown","metadata":{"id":"Rjkj103bICMs"},"source":["We can see that our RNN typically starts out correctly and sometimes is able to generate correct words but quickly goes on to generate junk as there is no long term dependencies.\n","\n","We will now implement a PyTorch LSTM to see how to improve upon this."]},{"cell_type":"markdown","metadata":{"id":"NPG6v0M7IVOB"},"source":["## Long Short Term Memory (LSTM)\n","\n","Let's take our recurrent neural network class that we have defined above and replace the simple RNN cell with one or even multiple stacked LSTM cells.\n","\n","If you want to go for the challenge you can try implementing this by hand similarly to the RNN cell we have defined. However, if you don't want to go through the tour-de-force exercise, you can go on ahead and use PyTorch's \"nn.LSTM()\" convenience method: https://pytorch.org/docs/stable/nn.html#torch.nn.LSTM\n","\n","You can try using a stack of 2 LSTM hidden layers to simply replace the RNN cell."]},{"cell_type":"code","metadata":{"id":"1dIQmxMEDs41","executionInfo":{"status":"ok","timestamp":1697190153999,"user_tz":-330,"elapsed":3,"user":{"displayName":"VARUN CHINDAGE (RA2111047010224)","userId":"15052075973333773834"}}},"source":["class LSTM(nn.Module):\n","\n","    def __init__(self, chars, device, n_hidden=256, n_layers=2, drop_prob=0.5):\n","        super().__init__()\n","\n","        self.device = device\n","\n","        self.drop_prob = drop_prob\n","        self.n_layers = n_layers\n","        self.n_hidden = n_hidden\n","\n","        # creating character dictionaries\n","        # we already have this code on the top, but giving it to our model\n","        # will be convenient for doing predictions later\n","        # i.e. doing conversions from text to integers to one-hot & vice-versa\n","        self.n_chars = len(chars)\n","        self.int2char = dict(enumerate(chars))\n","        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n","\n","        # define the LSTM\n","        # we no longer need to care about wieght initialization as PyTorch\n","        # will handle this for us now.\n","        # When defining PyTorch's nn.LSTM() set \"batch_first=True\" to assign\n","        # the batch size to the first dimension (instead of the sequence) to\n","        # stay consistent with our RNN implementation and re-use our code.\n","        self.lstm = nn.LSTM(self.n_chars, n_hidden, n_layers,\n","                            dropout=drop_prob, batch_first=True)\n","\n","        # define a dropout layer\n","        self.dropout = nn.Dropout(drop_prob)\n","\n","        # define the final, fully-connected output layer\n","        self.fc = nn.Linear(n_hidden, self.n_chars)\n","\n","    def forward(self, x, hidden):\n","        ''' Forward pass through the network.\n","            The inputs are x, and the hidden & cell state in a tuple. '''\n","\n","        # get the outputs and the new hidden states from the LSTM.\n","        # Note that the hidden variable now is a tuple of hidden and cell state\n","        # in contrast to the RNN that just had the hidden state.\n","        # Because we are using the PyTorch LSTM we do not need to implement\n","        # the loop anymore as the sequence will be handled internally.\n","        r_output, hidden = self.lstm(x, hidden)\n","\n","        # pass through a dropout layer\n","        out = self.dropout(r_output)\n","\n","        # Stack up the LSTM outputs using view so that we can process the last\n","        # layer in parallel\n","        out = out.contiguous().view(-1, self.n_hidden)\n","\n","        # Calculate fully connected layer output that yields our class vector\n","        out = self.fc(out)\n","\n","        return out, hidden\n","\n","    def init_hidden(self, batch_size=1):\n","        ''' Initializes hidden state '''\n","        # This is a convenience function so that we can initialize the hidden\n","        # states to zero when we start prediction on a sequence. Every further\n","        # step will then depend on the previous hidden states (c and h).\n","\n","        # Create a tuple of two new tensors with sizes\n","        # n_layers x batch_size x n_hidden, initialized to zero for the\n","        # LSTM hidden and cell states.\n","        weight = next(self.parameters()).data\n","\n","        hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device),\n","                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device))\n","\n","        return hidden"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GL9kCT6TK2QT"},"source":["We can use the exact same code to train our newly defined LSTM model. Let's try with the same amount of hidden units and 2 LSTM cells."]},{"cell_type":"code","metadata":{"id":"cW70FQwT2g_h","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f1aed13f-97b4-42fd-b8a9-e676c147511f","executionInfo":{"status":"ok","timestamp":1697194530026,"user_tz":-330,"elapsed":1849498,"user":{"displayName":"VARUN CHINDAGE (RA2111047010224)","userId":"15052075973333773834"}}},"source":["# Define the model\n","n_hidden=512\n","n_layers=2\n","\n","model = LSTM(chars, device, n_hidden, n_layers).to(device)\n","\n","# Declaring the hyperparameters\n","batch_size = 128\n","seq_length = 100\n","epochs = 1000 #was 300 # start with 50 or similar if you are debugging\n","# train much longer if you want good results\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","criterion = nn.CrossEntropyLoss()\n","\n","# train the model\n","train(model, encoded, device, optimizer, criterion, epochs=epochs,\n","      batch_size=batch_size, seq_length=seq_length)"],"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1/1000: Loss: 3.2991:\n","Epoch: 2/1000: Loss: 3.1680:\n","Epoch: 3/1000: Loss: 3.1328:\n","Epoch: 4/1000: Loss: 3.1210:\n","Epoch: 5/1000: Loss: 3.1160:\n","Epoch: 6/1000: Loss: 3.1068:\n","Epoch: 7/1000: Loss: 3.1065:\n","Epoch: 8/1000: Loss: 3.1033:\n","Epoch: 9/1000: Loss: 3.1015:\n","Epoch: 10/1000: Loss: 3.0952:\n","Epoch: 11/1000: Loss: 3.0917:\n","Epoch: 12/1000: Loss: 3.0860:\n","Epoch: 13/1000: Loss: 3.0692:\n","Epoch: 14/1000: Loss: 3.0295:\n","Epoch: 15/1000: Loss: 2.9558:\n","Epoch: 16/1000: Loss: 2.8888:\n","Epoch: 17/1000: Loss: 2.9368:\n","Epoch: 18/1000: Loss: 2.7831:\n","Epoch: 19/1000: Loss: 2.6908:\n","Epoch: 20/1000: Loss: 2.6173:\n","Epoch: 21/1000: Loss: 2.5532:\n","Epoch: 22/1000: Loss: 2.4869:\n","Epoch: 23/1000: Loss: 2.4409:\n","Epoch: 24/1000: Loss: 2.4035:\n","Epoch: 25/1000: Loss: 2.3742:\n","Epoch: 26/1000: Loss: 2.3671:\n","Epoch: 27/1000: Loss: 2.3409:\n","Epoch: 28/1000: Loss: 2.3284:\n","Epoch: 29/1000: Loss: 2.3023:\n","Epoch: 30/1000: Loss: 2.2786:\n","Epoch: 31/1000: Loss: 2.2590:\n","Epoch: 32/1000: Loss: 2.2422:\n","Epoch: 33/1000: Loss: 2.2331:\n","Epoch: 34/1000: Loss: 2.2057:\n","Epoch: 35/1000: Loss: 2.1809:\n","Epoch: 36/1000: Loss: 2.1788:\n","Epoch: 37/1000: Loss: 2.1584:\n","Epoch: 38/1000: Loss: 2.1435:\n","Epoch: 39/1000: Loss: 2.1131:\n","Epoch: 40/1000: Loss: 2.1046:\n","Epoch: 41/1000: Loss: 2.0709:\n","Epoch: 42/1000: Loss: 2.0730:\n","Epoch: 43/1000: Loss: 2.0509:\n","Epoch: 44/1000: Loss: 2.0282:\n","Epoch: 45/1000: Loss: 2.0114:\n","Epoch: 46/1000: Loss: 2.0032:\n","Epoch: 47/1000: Loss: 1.9894:\n","Epoch: 48/1000: Loss: 1.9655:\n","Epoch: 49/1000: Loss: 1.9583:\n","Epoch: 50/1000: Loss: 1.9430:\n","Epoch: 51/1000: Loss: 1.9349:\n","Epoch: 52/1000: Loss: 1.9176:\n","Epoch: 53/1000: Loss: 1.9134:\n","Epoch: 54/1000: Loss: 1.9079:\n","Epoch: 55/1000: Loss: 1.8902:\n","Epoch: 56/1000: Loss: 1.8835:\n","Epoch: 57/1000: Loss: 1.8662:\n","Epoch: 58/1000: Loss: 1.8547:\n","Epoch: 59/1000: Loss: 1.8477:\n","Epoch: 60/1000: Loss: 1.8341:\n","Epoch: 61/1000: Loss: 1.8259:\n","Epoch: 62/1000: Loss: 1.8215:\n","Epoch: 63/1000: Loss: 1.8045:\n","Epoch: 64/1000: Loss: 1.7918:\n","Epoch: 65/1000: Loss: 1.7963:\n","Epoch: 66/1000: Loss: 1.7787:\n","Epoch: 67/1000: Loss: 1.7686:\n","Epoch: 68/1000: Loss: 1.7560:\n","Epoch: 69/1000: Loss: 1.7496:\n","Epoch: 70/1000: Loss: 1.7417:\n","Epoch: 71/1000: Loss: 1.7391:\n","Epoch: 72/1000: Loss: 1.7346:\n","Epoch: 73/1000: Loss: 1.7240:\n","Epoch: 74/1000: Loss: 1.7110:\n","Epoch: 75/1000: Loss: 1.7082:\n","Epoch: 76/1000: Loss: 1.6993:\n","Epoch: 77/1000: Loss: 1.6864:\n","Epoch: 78/1000: Loss: 1.6798:\n","Epoch: 79/1000: Loss: 1.6719:\n","Epoch: 80/1000: Loss: 1.6679:\n","Epoch: 81/1000: Loss: 1.6635:\n","Epoch: 82/1000: Loss: 1.6596:\n","Epoch: 83/1000: Loss: 1.6457:\n","Epoch: 84/1000: Loss: 1.6428:\n","Epoch: 85/1000: Loss: 1.6402:\n","Epoch: 86/1000: Loss: 1.6227:\n","Epoch: 87/1000: Loss: 1.6222:\n","Epoch: 88/1000: Loss: 1.6216:\n","Epoch: 89/1000: Loss: 1.6100:\n","Epoch: 90/1000: Loss: 1.5916:\n","Epoch: 91/1000: Loss: 1.5953:\n","Epoch: 92/1000: Loss: 1.5753:\n","Epoch: 93/1000: Loss: 1.5733:\n","Epoch: 94/1000: Loss: 1.5598:\n","Epoch: 95/1000: Loss: 1.5544:\n","Epoch: 96/1000: Loss: 1.5564:\n","Epoch: 97/1000: Loss: 1.5450:\n","Epoch: 98/1000: Loss: 1.5400:\n","Epoch: 99/1000: Loss: 1.5258:\n","Epoch: 100/1000: Loss: 1.5150:\n","Epoch: 101/1000: Loss: 1.5177:\n","Epoch: 102/1000: Loss: 1.5049:\n","Epoch: 103/1000: Loss: 1.5056:\n","Epoch: 104/1000: Loss: 1.4853:\n","Epoch: 105/1000: Loss: 1.4926:\n","Epoch: 106/1000: Loss: 1.4832:\n","Epoch: 107/1000: Loss: 1.4640:\n","Epoch: 108/1000: Loss: 1.4725:\n","Epoch: 109/1000: Loss: 1.4460:\n","Epoch: 110/1000: Loss: 1.4547:\n","Epoch: 111/1000: Loss: 1.4239:\n","Epoch: 112/1000: Loss: 1.4223:\n","Epoch: 113/1000: Loss: 1.4142:\n","Epoch: 114/1000: Loss: 1.4054:\n","Epoch: 115/1000: Loss: 1.4059:\n","Epoch: 116/1000: Loss: 1.3911:\n","Epoch: 117/1000: Loss: 1.3940:\n","Epoch: 118/1000: Loss: 1.3754:\n","Epoch: 119/1000: Loss: 1.3734:\n","Epoch: 120/1000: Loss: 1.3649:\n","Epoch: 121/1000: Loss: 1.3558:\n","Epoch: 122/1000: Loss: 1.3601:\n","Epoch: 123/1000: Loss: 1.3292:\n","Epoch: 124/1000: Loss: 1.3322:\n","Epoch: 125/1000: Loss: 1.3220:\n","Epoch: 126/1000: Loss: 1.3266:\n","Epoch: 127/1000: Loss: 1.3101:\n","Epoch: 128/1000: Loss: 1.3077:\n","Epoch: 129/1000: Loss: 1.2886:\n","Epoch: 130/1000: Loss: 1.2732:\n","Epoch: 131/1000: Loss: 1.2631:\n","Epoch: 132/1000: Loss: 1.2769:\n","Epoch: 133/1000: Loss: 1.2703:\n","Epoch: 134/1000: Loss: 1.2415:\n","Epoch: 135/1000: Loss: 1.2230:\n","Epoch: 136/1000: Loss: 1.2085:\n","Epoch: 137/1000: Loss: 1.2034:\n","Epoch: 138/1000: Loss: 1.1968:\n","Epoch: 139/1000: Loss: 1.1890:\n","Epoch: 140/1000: Loss: 1.1895:\n","Epoch: 141/1000: Loss: 1.1785:\n","Epoch: 142/1000: Loss: 1.1750:\n","Epoch: 143/1000: Loss: 1.1557:\n","Epoch: 144/1000: Loss: 1.1703:\n","Epoch: 145/1000: Loss: 1.1556:\n","Epoch: 146/1000: Loss: 1.1569:\n","Epoch: 147/1000: Loss: 1.1215:\n","Epoch: 148/1000: Loss: 1.1090:\n","Epoch: 149/1000: Loss: 1.0996:\n","Epoch: 150/1000: Loss: 1.0718:\n","Epoch: 151/1000: Loss: 1.0617:\n","Epoch: 152/1000: Loss: 1.0451:\n","Epoch: 153/1000: Loss: 1.0584:\n","Epoch: 154/1000: Loss: 1.0259:\n","Epoch: 155/1000: Loss: 1.0192:\n","Epoch: 156/1000: Loss: 1.0079:\n","Epoch: 157/1000: Loss: 0.9976:\n","Epoch: 158/1000: Loss: 0.9906:\n","Epoch: 159/1000: Loss: 0.9791:\n","Epoch: 160/1000: Loss: 0.9649:\n","Epoch: 161/1000: Loss: 0.9580:\n","Epoch: 162/1000: Loss: 0.9499:\n","Epoch: 163/1000: Loss: 0.9362:\n","Epoch: 164/1000: Loss: 0.9309:\n","Epoch: 165/1000: Loss: 0.9326:\n","Epoch: 166/1000: Loss: 0.9094:\n","Epoch: 167/1000: Loss: 0.9127:\n","Epoch: 168/1000: Loss: 0.8836:\n","Epoch: 169/1000: Loss: 0.8849:\n","Epoch: 170/1000: Loss: 0.8816:\n","Epoch: 171/1000: Loss: 0.8774:\n","Epoch: 172/1000: Loss: 0.8766:\n","Epoch: 173/1000: Loss: 0.8517:\n","Epoch: 174/1000: Loss: 0.8375:\n","Epoch: 175/1000: Loss: 0.8312:\n","Epoch: 176/1000: Loss: 0.8304:\n","Epoch: 177/1000: Loss: 0.8053:\n","Epoch: 178/1000: Loss: 0.7859:\n","Epoch: 179/1000: Loss: 0.7905:\n","Epoch: 180/1000: Loss: 0.7767:\n","Epoch: 181/1000: Loss: 0.7735:\n","Epoch: 182/1000: Loss: 0.7609:\n","Epoch: 183/1000: Loss: 0.7543:\n","Epoch: 184/1000: Loss: 0.7284:\n","Epoch: 185/1000: Loss: 0.7262:\n","Epoch: 186/1000: Loss: 0.7117:\n","Epoch: 187/1000: Loss: 0.7336:\n","Epoch: 188/1000: Loss: 0.7251:\n","Epoch: 189/1000: Loss: 0.7194:\n","Epoch: 190/1000: Loss: 0.7271:\n","Epoch: 191/1000: Loss: 0.6839:\n","Epoch: 192/1000: Loss: 0.6934:\n","Epoch: 193/1000: Loss: 0.6754:\n","Epoch: 194/1000: Loss: 0.6541:\n","Epoch: 195/1000: Loss: 0.6406:\n","Epoch: 196/1000: Loss: 0.6275:\n","Epoch: 197/1000: Loss: 0.6422:\n","Epoch: 198/1000: Loss: 0.6647:\n","Epoch: 199/1000: Loss: 0.6431:\n","Epoch: 200/1000: Loss: 0.6220:\n","Epoch: 201/1000: Loss: 0.6117:\n","Epoch: 202/1000: Loss: 0.5911:\n","Epoch: 203/1000: Loss: 0.5801:\n","Epoch: 204/1000: Loss: 0.5610:\n","Epoch: 205/1000: Loss: 0.5526:\n","Epoch: 206/1000: Loss: 0.5400:\n","Epoch: 207/1000: Loss: 0.5417:\n","Epoch: 208/1000: Loss: 0.5215:\n","Epoch: 209/1000: Loss: 0.5126:\n","Epoch: 210/1000: Loss: 0.5227:\n","Epoch: 211/1000: Loss: 0.5253:\n","Epoch: 212/1000: Loss: 0.5017:\n","Epoch: 213/1000: Loss: 0.4976:\n","Epoch: 214/1000: Loss: 0.4974:\n","Epoch: 215/1000: Loss: 0.4788:\n","Epoch: 216/1000: Loss: 0.4866:\n","Epoch: 217/1000: Loss: 0.4789:\n","Epoch: 218/1000: Loss: 0.4835:\n","Epoch: 219/1000: Loss: 0.4885:\n","Epoch: 220/1000: Loss: 0.4670:\n","Epoch: 221/1000: Loss: 0.4715:\n","Epoch: 222/1000: Loss: 0.4606:\n","Epoch: 223/1000: Loss: 0.4315:\n","Epoch: 224/1000: Loss: 0.4205:\n","Epoch: 225/1000: Loss: 0.4338:\n","Epoch: 226/1000: Loss: 0.4227:\n","Epoch: 227/1000: Loss: 0.4018:\n","Epoch: 228/1000: Loss: 0.4043:\n","Epoch: 229/1000: Loss: 0.3856:\n","Epoch: 230/1000: Loss: 0.3856:\n","Epoch: 231/1000: Loss: 0.3926:\n","Epoch: 232/1000: Loss: 0.3751:\n","Epoch: 233/1000: Loss: 0.3925:\n","Epoch: 234/1000: Loss: 0.3677:\n","Epoch: 235/1000: Loss: 0.3777:\n","Epoch: 236/1000: Loss: 0.3677:\n","Epoch: 237/1000: Loss: 0.3670:\n","Epoch: 238/1000: Loss: 0.3578:\n","Epoch: 239/1000: Loss: 0.3606:\n","Epoch: 240/1000: Loss: 0.3478:\n","Epoch: 241/1000: Loss: 0.3485:\n","Epoch: 242/1000: Loss: 0.3292:\n","Epoch: 243/1000: Loss: 0.3422:\n","Epoch: 244/1000: Loss: 0.3215:\n","Epoch: 245/1000: Loss: 0.3184:\n","Epoch: 246/1000: Loss: 0.3182:\n","Epoch: 247/1000: Loss: 0.3007:\n","Epoch: 248/1000: Loss: 0.3070:\n","Epoch: 249/1000: Loss: 0.2972:\n","Epoch: 250/1000: Loss: 0.2902:\n","Epoch: 251/1000: Loss: 0.3048:\n","Epoch: 252/1000: Loss: 0.2953:\n","Epoch: 253/1000: Loss: 0.2854:\n","Epoch: 254/1000: Loss: 0.2762:\n","Epoch: 255/1000: Loss: 0.2733:\n","Epoch: 256/1000: Loss: 0.2864:\n","Epoch: 257/1000: Loss: 0.2740:\n","Epoch: 258/1000: Loss: 0.2751:\n","Epoch: 259/1000: Loss: 0.2718:\n","Epoch: 260/1000: Loss: 0.2615:\n","Epoch: 261/1000: Loss: 0.2720:\n","Epoch: 262/1000: Loss: 0.2510:\n","Epoch: 263/1000: Loss: 0.2638:\n","Epoch: 264/1000: Loss: 0.2572:\n","Epoch: 265/1000: Loss: 0.2582:\n","Epoch: 266/1000: Loss: 0.2501:\n","Epoch: 267/1000: Loss: 0.2507:\n","Epoch: 268/1000: Loss: 0.2456:\n","Epoch: 269/1000: Loss: 0.2379:\n","Epoch: 270/1000: Loss: 0.2284:\n","Epoch: 271/1000: Loss: 0.2383:\n","Epoch: 272/1000: Loss: 0.2337:\n","Epoch: 273/1000: Loss: 0.2297:\n","Epoch: 274/1000: Loss: 0.2221:\n","Epoch: 275/1000: Loss: 0.2300:\n","Epoch: 276/1000: Loss: 0.2284:\n","Epoch: 277/1000: Loss: 0.2190:\n","Epoch: 278/1000: Loss: 0.2234:\n","Epoch: 279/1000: Loss: 0.2123:\n","Epoch: 280/1000: Loss: 0.2162:\n","Epoch: 281/1000: Loss: 0.2143:\n","Epoch: 282/1000: Loss: 0.2062:\n","Epoch: 283/1000: Loss: 0.2079:\n","Epoch: 284/1000: Loss: 0.2017:\n","Epoch: 285/1000: Loss: 0.2047:\n","Epoch: 286/1000: Loss: 0.1909:\n","Epoch: 287/1000: Loss: 0.1890:\n","Epoch: 288/1000: Loss: 0.1844:\n","Epoch: 289/1000: Loss: 0.1829:\n","Epoch: 290/1000: Loss: 0.1840:\n","Epoch: 291/1000: Loss: 0.1849:\n","Epoch: 292/1000: Loss: 0.1801:\n","Epoch: 293/1000: Loss: 0.1797:\n","Epoch: 294/1000: Loss: 0.1806:\n","Epoch: 295/1000: Loss: 0.1756:\n","Epoch: 296/1000: Loss: 0.1790:\n","Epoch: 297/1000: Loss: 0.1852:\n","Epoch: 298/1000: Loss: 0.1662:\n","Epoch: 299/1000: Loss: 0.1773:\n","Epoch: 300/1000: Loss: 0.1703:\n","Epoch: 301/1000: Loss: 0.1656:\n","Epoch: 302/1000: Loss: 0.1617:\n","Epoch: 303/1000: Loss: 0.1652:\n","Epoch: 304/1000: Loss: 0.1618:\n","Epoch: 305/1000: Loss: 0.1599:\n","Epoch: 306/1000: Loss: 0.1654:\n","Epoch: 307/1000: Loss: 0.1658:\n","Epoch: 308/1000: Loss: 0.1557:\n","Epoch: 309/1000: Loss: 0.1532:\n","Epoch: 310/1000: Loss: 0.1615:\n","Epoch: 311/1000: Loss: 0.1554:\n","Epoch: 312/1000: Loss: 0.1532:\n","Epoch: 313/1000: Loss: 0.1581:\n","Epoch: 314/1000: Loss: 0.1525:\n","Epoch: 315/1000: Loss: 0.1500:\n","Epoch: 316/1000: Loss: 0.1495:\n","Epoch: 317/1000: Loss: 0.1508:\n","Epoch: 318/1000: Loss: 0.1526:\n","Epoch: 319/1000: Loss: 0.1450:\n","Epoch: 320/1000: Loss: 0.1434:\n","Epoch: 321/1000: Loss: 0.1491:\n","Epoch: 322/1000: Loss: 0.1461:\n","Epoch: 323/1000: Loss: 0.1422:\n","Epoch: 324/1000: Loss: 0.1369:\n","Epoch: 325/1000: Loss: 0.1319:\n","Epoch: 326/1000: Loss: 0.1394:\n","Epoch: 327/1000: Loss: 0.1351:\n","Epoch: 328/1000: Loss: 0.1280:\n","Epoch: 329/1000: Loss: 0.1357:\n","Epoch: 330/1000: Loss: 0.1360:\n","Epoch: 331/1000: Loss: 0.1274:\n","Epoch: 332/1000: Loss: 0.1283:\n","Epoch: 333/1000: Loss: 0.1270:\n","Epoch: 334/1000: Loss: 0.1277:\n","Epoch: 335/1000: Loss: 0.1248:\n","Epoch: 336/1000: Loss: 0.1251:\n","Epoch: 337/1000: Loss: 0.1234:\n","Epoch: 338/1000: Loss: 0.1235:\n","Epoch: 339/1000: Loss: 0.1303:\n","Epoch: 340/1000: Loss: 0.1248:\n","Epoch: 341/1000: Loss: 0.1314:\n","Epoch: 342/1000: Loss: 0.1195:\n","Epoch: 343/1000: Loss: 0.1208:\n","Epoch: 344/1000: Loss: 0.1206:\n","Epoch: 345/1000: Loss: 0.1139:\n","Epoch: 346/1000: Loss: 0.1126:\n","Epoch: 347/1000: Loss: 0.1239:\n","Epoch: 348/1000: Loss: 0.1196:\n","Epoch: 349/1000: Loss: 0.1129:\n","Epoch: 350/1000: Loss: 0.1052:\n","Epoch: 351/1000: Loss: 0.1219:\n","Epoch: 352/1000: Loss: 0.1080:\n","Epoch: 353/1000: Loss: 0.1082:\n","Epoch: 354/1000: Loss: 0.1149:\n","Epoch: 355/1000: Loss: 0.1061:\n","Epoch: 356/1000: Loss: 0.1078:\n","Epoch: 357/1000: Loss: 0.1061:\n","Epoch: 358/1000: Loss: 0.1080:\n","Epoch: 359/1000: Loss: 0.1022:\n","Epoch: 360/1000: Loss: 0.1003:\n","Epoch: 361/1000: Loss: 0.1050:\n","Epoch: 362/1000: Loss: 0.1035:\n","Epoch: 363/1000: Loss: 0.0994:\n","Epoch: 364/1000: Loss: 0.0968:\n","Epoch: 365/1000: Loss: 0.1027:\n","Epoch: 366/1000: Loss: 0.1012:\n","Epoch: 367/1000: Loss: 0.1062:\n","Epoch: 368/1000: Loss: 0.1006:\n","Epoch: 369/1000: Loss: 0.1017:\n","Epoch: 370/1000: Loss: 0.1009:\n","Epoch: 371/1000: Loss: 0.1014:\n","Epoch: 372/1000: Loss: 0.0954:\n","Epoch: 373/1000: Loss: 0.0959:\n","Epoch: 374/1000: Loss: 0.0945:\n","Epoch: 375/1000: Loss: 0.0972:\n","Epoch: 376/1000: Loss: 0.0976:\n","Epoch: 377/1000: Loss: 0.0910:\n","Epoch: 378/1000: Loss: 0.0923:\n","Epoch: 379/1000: Loss: 0.0911:\n","Epoch: 380/1000: Loss: 0.0928:\n","Epoch: 381/1000: Loss: 0.0996:\n","Epoch: 382/1000: Loss: 0.0950:\n","Epoch: 383/1000: Loss: 0.0911:\n","Epoch: 384/1000: Loss: 0.0977:\n","Epoch: 385/1000: Loss: 0.0967:\n","Epoch: 386/1000: Loss: 0.0914:\n","Epoch: 387/1000: Loss: 0.0917:\n","Epoch: 388/1000: Loss: 0.0932:\n","Epoch: 389/1000: Loss: 0.0897:\n","Epoch: 390/1000: Loss: 0.0965:\n","Epoch: 391/1000: Loss: 0.0911:\n","Epoch: 392/1000: Loss: 0.0867:\n","Epoch: 393/1000: Loss: 0.0871:\n","Epoch: 394/1000: Loss: 0.0878:\n","Epoch: 395/1000: Loss: 0.0878:\n","Epoch: 396/1000: Loss: 0.0839:\n","Epoch: 397/1000: Loss: 0.0877:\n","Epoch: 398/1000: Loss: 0.0848:\n","Epoch: 399/1000: Loss: 0.0846:\n","Epoch: 400/1000: Loss: 0.0869:\n","Epoch: 401/1000: Loss: 0.0844:\n","Epoch: 402/1000: Loss: 0.0836:\n","Epoch: 403/1000: Loss: 0.0834:\n","Epoch: 404/1000: Loss: 0.0805:\n","Epoch: 405/1000: Loss: 0.0834:\n","Epoch: 406/1000: Loss: 0.0849:\n","Epoch: 407/1000: Loss: 0.0835:\n","Epoch: 408/1000: Loss: 0.0806:\n","Epoch: 409/1000: Loss: 0.0826:\n","Epoch: 410/1000: Loss: 0.0817:\n","Epoch: 411/1000: Loss: 0.0787:\n","Epoch: 412/1000: Loss: 0.0777:\n","Epoch: 413/1000: Loss: 0.0818:\n","Epoch: 414/1000: Loss: 0.0827:\n","Epoch: 415/1000: Loss: 0.0764:\n","Epoch: 416/1000: Loss: 0.0761:\n","Epoch: 417/1000: Loss: 0.0734:\n","Epoch: 418/1000: Loss: 0.0730:\n","Epoch: 419/1000: Loss: 0.0745:\n","Epoch: 420/1000: Loss: 0.0754:\n","Epoch: 421/1000: Loss: 0.0756:\n","Epoch: 422/1000: Loss: 0.0795:\n","Epoch: 423/1000: Loss: 0.0743:\n","Epoch: 424/1000: Loss: 0.0846:\n","Epoch: 425/1000: Loss: 0.0726:\n","Epoch: 426/1000: Loss: 0.0795:\n","Epoch: 427/1000: Loss: 0.0739:\n","Epoch: 428/1000: Loss: 0.0727:\n","Epoch: 429/1000: Loss: 0.0766:\n","Epoch: 430/1000: Loss: 0.0791:\n","Epoch: 431/1000: Loss: 0.0719:\n","Epoch: 432/1000: Loss: 0.0724:\n","Epoch: 433/1000: Loss: 0.0720:\n","Epoch: 434/1000: Loss: 0.0716:\n","Epoch: 435/1000: Loss: 0.0708:\n","Epoch: 436/1000: Loss: 0.0702:\n","Epoch: 437/1000: Loss: 0.0694:\n","Epoch: 438/1000: Loss: 0.0730:\n","Epoch: 439/1000: Loss: 0.0723:\n","Epoch: 440/1000: Loss: 0.0651:\n","Epoch: 441/1000: Loss: 0.0701:\n","Epoch: 442/1000: Loss: 0.0733:\n","Epoch: 443/1000: Loss: 0.0702:\n","Epoch: 444/1000: Loss: 0.0637:\n","Epoch: 445/1000: Loss: 0.0754:\n","Epoch: 446/1000: Loss: 0.0722:\n","Epoch: 447/1000: Loss: 0.0686:\n","Epoch: 448/1000: Loss: 0.0751:\n","Epoch: 449/1000: Loss: 0.0697:\n","Epoch: 450/1000: Loss: 0.0702:\n","Epoch: 451/1000: Loss: 0.0798:\n","Epoch: 452/1000: Loss: 0.0682:\n","Epoch: 453/1000: Loss: 0.0707:\n","Epoch: 454/1000: Loss: 0.0741:\n","Epoch: 455/1000: Loss: 0.0699:\n","Epoch: 456/1000: Loss: 0.0659:\n","Epoch: 457/1000: Loss: 0.0702:\n","Epoch: 458/1000: Loss: 0.0764:\n","Epoch: 459/1000: Loss: 0.0678:\n","Epoch: 460/1000: Loss: 0.0695:\n","Epoch: 461/1000: Loss: 0.0660:\n","Epoch: 462/1000: Loss: 0.0588:\n","Epoch: 463/1000: Loss: 0.0622:\n","Epoch: 464/1000: Loss: 0.0683:\n","Epoch: 465/1000: Loss: 0.0652:\n","Epoch: 466/1000: Loss: 0.0658:\n","Epoch: 467/1000: Loss: 0.0624:\n","Epoch: 468/1000: Loss: 0.0673:\n","Epoch: 469/1000: Loss: 0.0630:\n","Epoch: 470/1000: Loss: 0.0625:\n","Epoch: 471/1000: Loss: 0.0593:\n","Epoch: 472/1000: Loss: 0.0630:\n","Epoch: 473/1000: Loss: 0.0573:\n","Epoch: 474/1000: Loss: 0.0593:\n","Epoch: 475/1000: Loss: 0.0632:\n","Epoch: 476/1000: Loss: 0.0562:\n","Epoch: 477/1000: Loss: 0.0579:\n","Epoch: 478/1000: Loss: 0.0580:\n","Epoch: 479/1000: Loss: 0.0618:\n","Epoch: 480/1000: Loss: 0.0624:\n","Epoch: 481/1000: Loss: 0.0623:\n","Epoch: 482/1000: Loss: 0.0547:\n","Epoch: 483/1000: Loss: 0.0578:\n","Epoch: 484/1000: Loss: 0.0553:\n","Epoch: 485/1000: Loss: 0.0583:\n","Epoch: 486/1000: Loss: 0.0601:\n","Epoch: 487/1000: Loss: 0.0589:\n","Epoch: 488/1000: Loss: 0.0619:\n","Epoch: 489/1000: Loss: 0.0586:\n","Epoch: 490/1000: Loss: 0.0537:\n","Epoch: 491/1000: Loss: 0.0563:\n","Epoch: 492/1000: Loss: 0.0533:\n","Epoch: 493/1000: Loss: 0.0603:\n","Epoch: 494/1000: Loss: 0.0544:\n","Epoch: 495/1000: Loss: 0.0584:\n","Epoch: 496/1000: Loss: 0.0601:\n","Epoch: 497/1000: Loss: 0.0561:\n","Epoch: 498/1000: Loss: 0.0629:\n","Epoch: 499/1000: Loss: 0.0610:\n","Epoch: 500/1000: Loss: 0.0593:\n","Epoch: 501/1000: Loss: 0.0576:\n","Epoch: 502/1000: Loss: 0.0611:\n","Epoch: 503/1000: Loss: 0.0550:\n","Epoch: 504/1000: Loss: 0.0550:\n","Epoch: 505/1000: Loss: 0.0555:\n","Epoch: 506/1000: Loss: 0.0591:\n","Epoch: 507/1000: Loss: 0.0527:\n","Epoch: 508/1000: Loss: 0.0553:\n","Epoch: 509/1000: Loss: 0.0562:\n","Epoch: 510/1000: Loss: 0.0584:\n","Epoch: 511/1000: Loss: 0.0581:\n","Epoch: 512/1000: Loss: 0.0545:\n","Epoch: 513/1000: Loss: 0.0538:\n","Epoch: 514/1000: Loss: 0.0552:\n","Epoch: 515/1000: Loss: 0.0552:\n","Epoch: 516/1000: Loss: 0.0545:\n","Epoch: 517/1000: Loss: 0.0515:\n","Epoch: 518/1000: Loss: 0.0507:\n","Epoch: 519/1000: Loss: 0.0528:\n","Epoch: 520/1000: Loss: 0.0537:\n","Epoch: 521/1000: Loss: 0.0525:\n","Epoch: 522/1000: Loss: 0.0516:\n","Epoch: 523/1000: Loss: 0.0583:\n","Epoch: 524/1000: Loss: 0.0544:\n","Epoch: 525/1000: Loss: 0.0533:\n","Epoch: 526/1000: Loss: 0.0589:\n","Epoch: 527/1000: Loss: 0.0575:\n","Epoch: 528/1000: Loss: 0.0519:\n","Epoch: 529/1000: Loss: 0.0539:\n","Epoch: 530/1000: Loss: 0.0506:\n","Epoch: 531/1000: Loss: 0.0550:\n","Epoch: 532/1000: Loss: 0.0557:\n","Epoch: 533/1000: Loss: 0.0529:\n","Epoch: 534/1000: Loss: 0.0529:\n","Epoch: 535/1000: Loss: 0.0526:\n","Epoch: 536/1000: Loss: 0.0521:\n","Epoch: 537/1000: Loss: 0.0555:\n","Epoch: 538/1000: Loss: 0.0510:\n","Epoch: 539/1000: Loss: 0.0484:\n","Epoch: 540/1000: Loss: 0.0508:\n","Epoch: 541/1000: Loss: 0.0501:\n","Epoch: 542/1000: Loss: 0.0469:\n","Epoch: 543/1000: Loss: 0.0467:\n","Epoch: 544/1000: Loss: 0.0521:\n","Epoch: 545/1000: Loss: 0.0486:\n","Epoch: 546/1000: Loss: 0.0487:\n","Epoch: 547/1000: Loss: 0.0481:\n","Epoch: 548/1000: Loss: 0.0519:\n","Epoch: 549/1000: Loss: 0.0482:\n","Epoch: 550/1000: Loss: 0.0509:\n","Epoch: 551/1000: Loss: 0.0487:\n","Epoch: 552/1000: Loss: 0.0492:\n","Epoch: 553/1000: Loss: 0.0475:\n","Epoch: 554/1000: Loss: 0.0498:\n","Epoch: 555/1000: Loss: 0.0500:\n","Epoch: 556/1000: Loss: 0.0503:\n","Epoch: 557/1000: Loss: 0.0493:\n","Epoch: 558/1000: Loss: 0.0509:\n","Epoch: 559/1000: Loss: 0.0499:\n","Epoch: 560/1000: Loss: 0.0476:\n","Epoch: 561/1000: Loss: 0.0526:\n","Epoch: 562/1000: Loss: 0.0466:\n","Epoch: 563/1000: Loss: 0.0469:\n","Epoch: 564/1000: Loss: 0.0493:\n","Epoch: 565/1000: Loss: 0.0507:\n","Epoch: 566/1000: Loss: 0.0532:\n","Epoch: 567/1000: Loss: 0.0461:\n","Epoch: 568/1000: Loss: 0.0480:\n","Epoch: 569/1000: Loss: 0.0494:\n","Epoch: 570/1000: Loss: 0.0476:\n","Epoch: 571/1000: Loss: 0.0507:\n","Epoch: 572/1000: Loss: 0.0441:\n","Epoch: 573/1000: Loss: 0.0495:\n","Epoch: 574/1000: Loss: 0.0440:\n","Epoch: 575/1000: Loss: 0.0449:\n","Epoch: 576/1000: Loss: 0.0444:\n","Epoch: 577/1000: Loss: 0.0482:\n","Epoch: 578/1000: Loss: 0.0445:\n","Epoch: 579/1000: Loss: 0.0491:\n","Epoch: 580/1000: Loss: 0.0446:\n","Epoch: 581/1000: Loss: 0.0454:\n","Epoch: 582/1000: Loss: 0.0490:\n","Epoch: 583/1000: Loss: 0.0492:\n","Epoch: 584/1000: Loss: 0.0500:\n","Epoch: 585/1000: Loss: 0.0459:\n","Epoch: 586/1000: Loss: 0.0426:\n","Epoch: 587/1000: Loss: 0.0445:\n","Epoch: 588/1000: Loss: 0.0463:\n","Epoch: 589/1000: Loss: 0.0502:\n","Epoch: 590/1000: Loss: 0.0448:\n","Epoch: 591/1000: Loss: 0.0428:\n","Epoch: 592/1000: Loss: 0.0439:\n","Epoch: 593/1000: Loss: 0.0443:\n","Epoch: 594/1000: Loss: 0.0459:\n","Epoch: 595/1000: Loss: 0.0472:\n","Epoch: 596/1000: Loss: 0.0416:\n","Epoch: 597/1000: Loss: 0.0429:\n","Epoch: 598/1000: Loss: 0.0509:\n","Epoch: 599/1000: Loss: 0.0500:\n","Epoch: 600/1000: Loss: 0.0556:\n","Epoch: 601/1000: Loss: 0.0567:\n","Epoch: 602/1000: Loss: 0.0517:\n","Epoch: 603/1000: Loss: 0.0510:\n","Epoch: 604/1000: Loss: 0.0509:\n","Epoch: 605/1000: Loss: 0.0435:\n","Epoch: 606/1000: Loss: 0.0434:\n","Epoch: 607/1000: Loss: 0.0442:\n","Epoch: 608/1000: Loss: 0.0446:\n","Epoch: 609/1000: Loss: 0.0457:\n","Epoch: 610/1000: Loss: 0.0430:\n","Epoch: 611/1000: Loss: 0.0450:\n","Epoch: 612/1000: Loss: 0.0412:\n","Epoch: 613/1000: Loss: 0.0440:\n","Epoch: 614/1000: Loss: 0.0423:\n","Epoch: 615/1000: Loss: 0.0412:\n","Epoch: 616/1000: Loss: 0.0412:\n","Epoch: 617/1000: Loss: 0.0430:\n","Epoch: 618/1000: Loss: 0.0414:\n","Epoch: 619/1000: Loss: 0.0405:\n","Epoch: 620/1000: Loss: 0.0409:\n","Epoch: 621/1000: Loss: 0.0450:\n","Epoch: 622/1000: Loss: 0.0422:\n","Epoch: 623/1000: Loss: 0.0399:\n","Epoch: 624/1000: Loss: 0.0385:\n","Epoch: 625/1000: Loss: 0.0430:\n","Epoch: 626/1000: Loss: 0.0418:\n","Epoch: 627/1000: Loss: 0.0379:\n","Epoch: 628/1000: Loss: 0.0400:\n","Epoch: 629/1000: Loss: 0.0434:\n","Epoch: 630/1000: Loss: 0.0455:\n","Epoch: 631/1000: Loss: 0.0374:\n","Epoch: 632/1000: Loss: 0.0439:\n","Epoch: 633/1000: Loss: 0.0407:\n","Epoch: 634/1000: Loss: 0.0429:\n","Epoch: 635/1000: Loss: 0.0422:\n","Epoch: 636/1000: Loss: 0.0394:\n","Epoch: 637/1000: Loss: 0.0417:\n","Epoch: 638/1000: Loss: 0.0399:\n","Epoch: 639/1000: Loss: 0.0418:\n","Epoch: 640/1000: Loss: 0.0408:\n","Epoch: 641/1000: Loss: 0.0395:\n","Epoch: 642/1000: Loss: 0.0417:\n","Epoch: 643/1000: Loss: 0.0449:\n","Epoch: 644/1000: Loss: 0.0365:\n","Epoch: 645/1000: Loss: 0.0350:\n","Epoch: 646/1000: Loss: 0.0412:\n","Epoch: 647/1000: Loss: 0.0449:\n","Epoch: 648/1000: Loss: 0.0366:\n","Epoch: 649/1000: Loss: 0.0375:\n","Epoch: 650/1000: Loss: 0.0437:\n","Epoch: 651/1000: Loss: 0.0392:\n","Epoch: 652/1000: Loss: 0.0427:\n","Epoch: 653/1000: Loss: 0.0381:\n","Epoch: 654/1000: Loss: 0.0394:\n","Epoch: 655/1000: Loss: 0.0365:\n","Epoch: 656/1000: Loss: 0.0397:\n","Epoch: 657/1000: Loss: 0.0363:\n","Epoch: 658/1000: Loss: 0.0360:\n","Epoch: 659/1000: Loss: 0.0372:\n","Epoch: 660/1000: Loss: 0.0417:\n","Epoch: 661/1000: Loss: 0.0443:\n","Epoch: 662/1000: Loss: 0.0578:\n","Epoch: 663/1000: Loss: 0.0523:\n","Epoch: 664/1000: Loss: 0.0463:\n","Epoch: 665/1000: Loss: 0.0561:\n","Epoch: 666/1000: Loss: 0.0857:\n","Epoch: 667/1000: Loss: 0.0661:\n","Epoch: 668/1000: Loss: 0.0735:\n","Epoch: 669/1000: Loss: 0.0643:\n","Epoch: 670/1000: Loss: 0.0594:\n","Epoch: 671/1000: Loss: 0.0520:\n","Epoch: 672/1000: Loss: 0.0451:\n","Epoch: 673/1000: Loss: 0.0514:\n","Epoch: 674/1000: Loss: 0.0503:\n","Epoch: 675/1000: Loss: 0.0443:\n","Epoch: 676/1000: Loss: 0.0422:\n","Epoch: 677/1000: Loss: 0.0403:\n","Epoch: 678/1000: Loss: 0.0398:\n","Epoch: 679/1000: Loss: 0.0364:\n","Epoch: 680/1000: Loss: 0.0354:\n","Epoch: 681/1000: Loss: 0.0447:\n","Epoch: 682/1000: Loss: 0.0415:\n","Epoch: 683/1000: Loss: 0.0363:\n","Epoch: 684/1000: Loss: 0.0356:\n","Epoch: 685/1000: Loss: 0.0392:\n","Epoch: 686/1000: Loss: 0.0331:\n","Epoch: 687/1000: Loss: 0.0363:\n","Epoch: 688/1000: Loss: 0.0351:\n","Epoch: 689/1000: Loss: 0.0354:\n","Epoch: 690/1000: Loss: 0.0355:\n","Epoch: 691/1000: Loss: 0.0367:\n","Epoch: 692/1000: Loss: 0.0307:\n","Epoch: 693/1000: Loss: 0.0363:\n","Epoch: 694/1000: Loss: 0.0372:\n","Epoch: 695/1000: Loss: 0.0383:\n","Epoch: 696/1000: Loss: 0.0336:\n","Epoch: 697/1000: Loss: 0.0327:\n","Epoch: 698/1000: Loss: 0.0333:\n","Epoch: 699/1000: Loss: 0.0323:\n","Epoch: 700/1000: Loss: 0.0346:\n","Epoch: 701/1000: Loss: 0.0347:\n","Epoch: 702/1000: Loss: 0.0320:\n","Epoch: 703/1000: Loss: 0.0348:\n","Epoch: 704/1000: Loss: 0.0369:\n","Epoch: 705/1000: Loss: 0.0316:\n","Epoch: 706/1000: Loss: 0.0378:\n","Epoch: 707/1000: Loss: 0.0332:\n","Epoch: 708/1000: Loss: 0.0361:\n","Epoch: 709/1000: Loss: 0.0331:\n","Epoch: 710/1000: Loss: 0.0380:\n","Epoch: 711/1000: Loss: 0.0334:\n","Epoch: 712/1000: Loss: 0.0346:\n","Epoch: 713/1000: Loss: 0.0323:\n","Epoch: 714/1000: Loss: 0.0364:\n","Epoch: 715/1000: Loss: 0.0382:\n","Epoch: 716/1000: Loss: 0.0347:\n","Epoch: 717/1000: Loss: 0.0333:\n","Epoch: 718/1000: Loss: 0.0392:\n","Epoch: 719/1000: Loss: 0.0350:\n","Epoch: 720/1000: Loss: 0.0336:\n","Epoch: 721/1000: Loss: 0.0407:\n","Epoch: 722/1000: Loss: 0.0353:\n","Epoch: 723/1000: Loss: 0.0328:\n","Epoch: 724/1000: Loss: 0.0359:\n","Epoch: 725/1000: Loss: 0.0344:\n","Epoch: 726/1000: Loss: 0.0353:\n","Epoch: 727/1000: Loss: 0.0359:\n","Epoch: 728/1000: Loss: 0.0341:\n","Epoch: 729/1000: Loss: 0.0367:\n","Epoch: 730/1000: Loss: 0.0356:\n","Epoch: 731/1000: Loss: 0.0367:\n","Epoch: 732/1000: Loss: 0.0382:\n","Epoch: 733/1000: Loss: 0.0358:\n","Epoch: 734/1000: Loss: 0.0363:\n","Epoch: 735/1000: Loss: 0.0316:\n","Epoch: 736/1000: Loss: 0.0333:\n","Epoch: 737/1000: Loss: 0.0323:\n","Epoch: 738/1000: Loss: 0.0335:\n","Epoch: 739/1000: Loss: 0.0282:\n","Epoch: 740/1000: Loss: 0.0374:\n","Epoch: 741/1000: Loss: 0.0341:\n","Epoch: 742/1000: Loss: 0.0347:\n","Epoch: 743/1000: Loss: 0.0318:\n","Epoch: 744/1000: Loss: 0.0311:\n","Epoch: 745/1000: Loss: 0.0334:\n","Epoch: 746/1000: Loss: 0.0356:\n","Epoch: 747/1000: Loss: 0.0338:\n","Epoch: 748/1000: Loss: 0.0346:\n","Epoch: 749/1000: Loss: 0.0336:\n","Epoch: 750/1000: Loss: 0.0311:\n","Epoch: 751/1000: Loss: 0.0301:\n","Epoch: 752/1000: Loss: 0.0338:\n","Epoch: 753/1000: Loss: 0.0325:\n","Epoch: 754/1000: Loss: 0.0346:\n","Epoch: 755/1000: Loss: 0.0361:\n","Epoch: 756/1000: Loss: 0.0343:\n","Epoch: 757/1000: Loss: 0.0330:\n","Epoch: 758/1000: Loss: 0.0362:\n","Epoch: 759/1000: Loss: 0.0320:\n","Epoch: 760/1000: Loss: 0.0305:\n","Epoch: 761/1000: Loss: 0.0358:\n","Epoch: 762/1000: Loss: 0.0335:\n","Epoch: 763/1000: Loss: 0.0388:\n","Epoch: 764/1000: Loss: 0.0356:\n","Epoch: 765/1000: Loss: 0.0319:\n","Epoch: 766/1000: Loss: 0.0339:\n","Epoch: 767/1000: Loss: 0.0354:\n","Epoch: 768/1000: Loss: 0.0348:\n","Epoch: 769/1000: Loss: 0.0329:\n","Epoch: 770/1000: Loss: 0.0349:\n","Epoch: 771/1000: Loss: 0.0373:\n","Epoch: 772/1000: Loss: 0.0352:\n","Epoch: 773/1000: Loss: 0.0342:\n","Epoch: 774/1000: Loss: 0.0347:\n","Epoch: 775/1000: Loss: 0.0376:\n","Epoch: 776/1000: Loss: 0.0356:\n","Epoch: 777/1000: Loss: 0.0348:\n","Epoch: 778/1000: Loss: 0.0301:\n","Epoch: 779/1000: Loss: 0.0315:\n","Epoch: 780/1000: Loss: 0.0311:\n","Epoch: 781/1000: Loss: 0.0371:\n","Epoch: 782/1000: Loss: 0.0354:\n","Epoch: 783/1000: Loss: 0.0322:\n","Epoch: 784/1000: Loss: 0.0301:\n","Epoch: 785/1000: Loss: 0.0273:\n","Epoch: 786/1000: Loss: 0.0337:\n","Epoch: 787/1000: Loss: 0.0360:\n","Epoch: 788/1000: Loss: 0.0310:\n","Epoch: 789/1000: Loss: 0.0320:\n","Epoch: 790/1000: Loss: 0.0282:\n","Epoch: 791/1000: Loss: 0.0323:\n","Epoch: 792/1000: Loss: 0.0287:\n","Epoch: 793/1000: Loss: 0.0287:\n","Epoch: 794/1000: Loss: 0.0309:\n","Epoch: 795/1000: Loss: 0.0302:\n","Epoch: 796/1000: Loss: 0.0291:\n","Epoch: 797/1000: Loss: 0.0339:\n","Epoch: 798/1000: Loss: 0.0282:\n","Epoch: 799/1000: Loss: 0.0339:\n","Epoch: 800/1000: Loss: 0.0300:\n","Epoch: 801/1000: Loss: 0.0285:\n","Epoch: 802/1000: Loss: 0.0324:\n","Epoch: 803/1000: Loss: 0.0303:\n","Epoch: 804/1000: Loss: 0.0338:\n","Epoch: 805/1000: Loss: 0.0347:\n","Epoch: 806/1000: Loss: 0.0299:\n","Epoch: 807/1000: Loss: 0.0308:\n","Epoch: 808/1000: Loss: 0.0305:\n","Epoch: 809/1000: Loss: 0.0270:\n","Epoch: 810/1000: Loss: 0.0278:\n","Epoch: 811/1000: Loss: 0.0286:\n","Epoch: 812/1000: Loss: 0.0280:\n","Epoch: 813/1000: Loss: 0.0345:\n","Epoch: 814/1000: Loss: 0.0283:\n","Epoch: 815/1000: Loss: 0.0316:\n","Epoch: 816/1000: Loss: 0.0286:\n","Epoch: 817/1000: Loss: 0.0326:\n","Epoch: 818/1000: Loss: 0.0326:\n","Epoch: 819/1000: Loss: 0.0319:\n","Epoch: 820/1000: Loss: 0.0306:\n","Epoch: 821/1000: Loss: 0.0299:\n","Epoch: 822/1000: Loss: 0.0296:\n","Epoch: 823/1000: Loss: 0.0328:\n","Epoch: 824/1000: Loss: 0.0308:\n","Epoch: 825/1000: Loss: 0.0282:\n","Epoch: 826/1000: Loss: 0.0321:\n","Epoch: 827/1000: Loss: 0.0327:\n","Epoch: 828/1000: Loss: 0.0311:\n","Epoch: 829/1000: Loss: 0.0319:\n","Epoch: 830/1000: Loss: 0.0337:\n","Epoch: 831/1000: Loss: 0.0340:\n","Epoch: 832/1000: Loss: 0.0316:\n","Epoch: 833/1000: Loss: 0.0329:\n","Epoch: 834/1000: Loss: 0.0305:\n","Epoch: 835/1000: Loss: 0.0304:\n","Epoch: 836/1000: Loss: 0.0332:\n","Epoch: 837/1000: Loss: 0.0332:\n","Epoch: 838/1000: Loss: 0.0318:\n","Epoch: 839/1000: Loss: 0.0338:\n","Epoch: 840/1000: Loss: 0.0346:\n","Epoch: 841/1000: Loss: 0.0300:\n","Epoch: 842/1000: Loss: 0.0307:\n","Epoch: 843/1000: Loss: 0.0315:\n","Epoch: 844/1000: Loss: 0.0309:\n","Epoch: 845/1000: Loss: 0.0294:\n","Epoch: 846/1000: Loss: 0.0301:\n","Epoch: 847/1000: Loss: 0.0297:\n","Epoch: 848/1000: Loss: 0.0300:\n","Epoch: 849/1000: Loss: 0.0345:\n","Epoch: 850/1000: Loss: 0.0336:\n","Epoch: 851/1000: Loss: 0.0306:\n","Epoch: 852/1000: Loss: 0.0325:\n","Epoch: 853/1000: Loss: 0.0298:\n","Epoch: 854/1000: Loss: 0.0340:\n","Epoch: 855/1000: Loss: 0.0279:\n","Epoch: 856/1000: Loss: 0.0362:\n","Epoch: 857/1000: Loss: 0.0313:\n","Epoch: 858/1000: Loss: 0.0337:\n","Epoch: 859/1000: Loss: 0.0285:\n","Epoch: 860/1000: Loss: 0.0288:\n","Epoch: 861/1000: Loss: 0.0342:\n","Epoch: 862/1000: Loss: 0.0288:\n","Epoch: 863/1000: Loss: 0.0326:\n","Epoch: 864/1000: Loss: 0.0310:\n","Epoch: 865/1000: Loss: 0.0308:\n","Epoch: 866/1000: Loss: 0.0321:\n","Epoch: 867/1000: Loss: 0.0285:\n","Epoch: 868/1000: Loss: 0.0287:\n","Epoch: 869/1000: Loss: 0.0297:\n","Epoch: 870/1000: Loss: 0.0297:\n","Epoch: 871/1000: Loss: 0.0295:\n","Epoch: 872/1000: Loss: 0.0266:\n","Epoch: 873/1000: Loss: 0.0290:\n","Epoch: 874/1000: Loss: 0.0311:\n","Epoch: 875/1000: Loss: 0.0300:\n","Epoch: 876/1000: Loss: 0.0287:\n","Epoch: 877/1000: Loss: 0.0279:\n","Epoch: 878/1000: Loss: 0.0300:\n","Epoch: 879/1000: Loss: 0.0312:\n","Epoch: 880/1000: Loss: 0.0290:\n","Epoch: 881/1000: Loss: 0.0318:\n","Epoch: 882/1000: Loss: 0.0257:\n","Epoch: 883/1000: Loss: 0.0322:\n","Epoch: 884/1000: Loss: 0.0292:\n","Epoch: 885/1000: Loss: 0.0305:\n","Epoch: 886/1000: Loss: 0.0311:\n","Epoch: 887/1000: Loss: 0.0303:\n","Epoch: 888/1000: Loss: 0.0292:\n","Epoch: 889/1000: Loss: 0.0301:\n","Epoch: 890/1000: Loss: 0.0285:\n","Epoch: 891/1000: Loss: 0.0263:\n","Epoch: 892/1000: Loss: 0.0315:\n","Epoch: 893/1000: Loss: 0.0281:\n","Epoch: 894/1000: Loss: 0.0291:\n","Epoch: 895/1000: Loss: 0.0364:\n","Epoch: 896/1000: Loss: 0.0323:\n","Epoch: 897/1000: Loss: 0.0318:\n","Epoch: 898/1000: Loss: 0.0314:\n","Epoch: 899/1000: Loss: 0.0313:\n","Epoch: 900/1000: Loss: 0.0287:\n","Epoch: 901/1000: Loss: 0.0332:\n","Epoch: 902/1000: Loss: 0.0328:\n","Epoch: 903/1000: Loss: 0.0280:\n","Epoch: 904/1000: Loss: 0.0314:\n","Epoch: 905/1000: Loss: 0.0294:\n","Epoch: 906/1000: Loss: 0.0274:\n","Epoch: 907/1000: Loss: 0.0293:\n","Epoch: 908/1000: Loss: 0.0314:\n","Epoch: 909/1000: Loss: 0.0275:\n","Epoch: 910/1000: Loss: 0.0304:\n","Epoch: 911/1000: Loss: 0.0336:\n","Epoch: 912/1000: Loss: 0.0346:\n","Epoch: 913/1000: Loss: 0.0282:\n","Epoch: 914/1000: Loss: 0.0332:\n","Epoch: 915/1000: Loss: 0.0292:\n","Epoch: 916/1000: Loss: 0.0242:\n","Epoch: 917/1000: Loss: 0.0291:\n","Epoch: 918/1000: Loss: 0.0302:\n","Epoch: 919/1000: Loss: 0.0269:\n","Epoch: 920/1000: Loss: 0.0298:\n","Epoch: 921/1000: Loss: 0.0308:\n","Epoch: 922/1000: Loss: 0.0294:\n","Epoch: 923/1000: Loss: 0.0297:\n","Epoch: 924/1000: Loss: 0.0254:\n","Epoch: 925/1000: Loss: 0.0272:\n","Epoch: 926/1000: Loss: 0.0291:\n","Epoch: 927/1000: Loss: 0.0265:\n","Epoch: 928/1000: Loss: 0.0258:\n","Epoch: 929/1000: Loss: 0.0236:\n","Epoch: 930/1000: Loss: 0.0278:\n","Epoch: 931/1000: Loss: 0.0304:\n","Epoch: 932/1000: Loss: 0.0277:\n","Epoch: 933/1000: Loss: 0.0297:\n","Epoch: 934/1000: Loss: 0.0257:\n","Epoch: 935/1000: Loss: 0.0274:\n","Epoch: 936/1000: Loss: 0.0276:\n","Epoch: 937/1000: Loss: 0.0289:\n","Epoch: 938/1000: Loss: 0.0273:\n","Epoch: 939/1000: Loss: 0.0304:\n","Epoch: 940/1000: Loss: 0.0303:\n","Epoch: 941/1000: Loss: 0.0305:\n","Epoch: 942/1000: Loss: 0.0310:\n","Epoch: 943/1000: Loss: 0.0281:\n","Epoch: 944/1000: Loss: 0.0276:\n","Epoch: 945/1000: Loss: 0.0278:\n","Epoch: 946/1000: Loss: 0.0278:\n","Epoch: 947/1000: Loss: 0.0247:\n","Epoch: 948/1000: Loss: 0.0264:\n","Epoch: 949/1000: Loss: 0.0301:\n","Epoch: 950/1000: Loss: 0.0261:\n","Epoch: 951/1000: Loss: 0.0302:\n","Epoch: 952/1000: Loss: 0.0276:\n","Epoch: 953/1000: Loss: 0.0269:\n","Epoch: 954/1000: Loss: 0.0240:\n","Epoch: 955/1000: Loss: 0.0286:\n","Epoch: 956/1000: Loss: 0.0274:\n","Epoch: 957/1000: Loss: 0.0242:\n","Epoch: 958/1000: Loss: 0.0291:\n","Epoch: 959/1000: Loss: 0.0265:\n","Epoch: 960/1000: Loss: 0.0267:\n","Epoch: 961/1000: Loss: 0.0232:\n","Epoch: 962/1000: Loss: 0.0267:\n","Epoch: 963/1000: Loss: 0.0254:\n","Epoch: 964/1000: Loss: 0.0248:\n","Epoch: 965/1000: Loss: 0.0239:\n","Epoch: 966/1000: Loss: 0.0289:\n","Epoch: 967/1000: Loss: 0.0294:\n","Epoch: 968/1000: Loss: 0.0257:\n","Epoch: 969/1000: Loss: 0.0253:\n","Epoch: 970/1000: Loss: 0.0290:\n","Epoch: 971/1000: Loss: 0.0266:\n","Epoch: 972/1000: Loss: 0.0231:\n","Epoch: 973/1000: Loss: 0.0264:\n","Epoch: 974/1000: Loss: 0.0260:\n","Epoch: 975/1000: Loss: 0.0283:\n","Epoch: 976/1000: Loss: 0.0235:\n","Epoch: 977/1000: Loss: 0.0261:\n","Epoch: 978/1000: Loss: 0.0227:\n","Epoch: 979/1000: Loss: 0.0213:\n","Epoch: 980/1000: Loss: 0.0250:\n","Epoch: 981/1000: Loss: 0.0268:\n","Epoch: 982/1000: Loss: 0.0262:\n","Epoch: 983/1000: Loss: 0.0245:\n","Epoch: 984/1000: Loss: 0.0258:\n","Epoch: 985/1000: Loss: 0.0237:\n","Epoch: 986/1000: Loss: 0.0235:\n","Epoch: 987/1000: Loss: 0.0258:\n","Epoch: 988/1000: Loss: 0.0255:\n","Epoch: 989/1000: Loss: 0.0259:\n","Epoch: 990/1000: Loss: 0.0243:\n","Epoch: 991/1000: Loss: 0.0271:\n","Epoch: 992/1000: Loss: 0.0239:\n","Epoch: 993/1000: Loss: 0.0268:\n","Epoch: 994/1000: Loss: 0.0251:\n","Epoch: 995/1000: Loss: 0.0326:\n","Epoch: 996/1000: Loss: 0.0245:\n","Epoch: 997/1000: Loss: 0.0273:\n","Epoch: 998/1000: Loss: 0.0271:\n","Epoch: 999/1000: Loss: 0.0236:\n","Epoch: 1000/1000: Loss: 0.0232:\n"]}]},{"cell_type":"markdown","metadata":{"id":"OTznlDdJK81w"},"source":["We can observe that our model is able to achieve a much lower loss than before with our simple RNN implementation. This should now also be reflected when we generate/sample new sonnets.\n","\n","Again, you can observe that the loss still continues to improve, even after 300 epochs. For the best results, try training the model longer."]},{"cell_type":"code","metadata":{"id":"m4pmGQ5J2lmJ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"590be330-18da-48d1-9532-2ee860743bc8","executionInfo":{"status":"ok","timestamp":1697194849151,"user_tz":-330,"elapsed":1353,"user":{"displayName":"VARUN CHINDAGE (RA2111047010224)","userId":"15052075973333773834"}}},"source":["# Generating new text\n","print(sample(model, 1004, device, prime='butter chicken', top_k=15))"],"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["butter chicken geathe with the michtense.\n","\n","The correst contert's and most precious jewel.\n","Yet, in good faith, some say that thee behold,\n","Thy face hath not the power to make love groan;\n","To say they err I dare no, leaves lest are;\n","For thou art betome of your a modery;\n","And the ciret be stall gave thet you shell,\n","I sin upen messed a conmen sor seal,\n","And therefy look doth sweet in our lise,\n","Shale preace there-for the some a forber shime\n","Thou art my slower, yet, but so child I fear,\n","That on the askea  ton oroules farse.\n","If thou contracted to thine own bright eyes,\n","Feed'st thy light's flame with self-substantial fuel,\n","Making a famine where abundance lies,\n","Thy self thy foe, to thy sweet self too cruel:\n","Thou that art now the world's fresh ornament,\n","And only herald to the gaudy spring,\n","Within thine own bud buriest thy content,\n","And, tender churl, mak'st waste in niggarding:\n","Pity the world, or else this glutton be,\n","To eat the world's due, by the grave and thee.\n","\n","When forty winters shall besiege thy brow,\n","And dig dee\n"]}]},{"cell_type":"markdown","metadata":{"id":"AXT-5Y-rLRKt"},"source":["Arguably there is still discrepancy to the original Shakespeare texts in our just generated examples. However, if we compare this output to our RNN's output, we can see that the LSTM is able to achieve much more consistency in generating proper words as well as sometimes portions of sentences that have improved in terms of grammar. There now seems to be more overall structure."]}]}